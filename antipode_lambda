#!/usr/bin/env python3

from pathlib import Path
from pprint import pprint
import os
import sys
import argparse
import random
import string
import multiprocessing as mp
import json
from datetime import datetime
from plumbum import FG, BG
import time

#--------------
# HELPERS
#--------------

def _dict_reverse_lookup(d, v):
  return list(d.keys())[list(d.values()).index(v)]

# ORIGINAL:
#def _dict_reverse_lookup(d, v):
#  return d.keys()[d.values().index(v)]

def _generate_payload(i):
  key = str(i) + ''.join(random.choices(string.ascii_uppercase, k=1)) + ''.join(random.choices(string.ascii_uppercase + string.digits, k=NOTIFICATION_KEY_LEN - len(str(i)) - 1))
  return {
      'i': i,
      'key': key,
      'client_sent_at': datetime.utcnow().timestamp()
    }

def _load_yaml(path):
  import yaml
  with open(path, 'r') as f:
    return yaml.safe_load(f) or {}

def _put_last(k,v):
  import yaml
  doc = {}
  # if file exists parse yaml otherwise create empty dict to write to
  if Path(LAST_INFO_FILE).exists():
    doc = _load_yaml(LAST_INFO_FILE)
  # write new value and save to file
  doc[k] = v
  with open(LAST_INFO_FILE, 'w+') as f:
      yaml.safe_dump(doc, f, default_flow_style=False)

def _get_last(k):
  import yaml
  doc = _load_yaml(LAST_INFO_FILE)
  return doc.get(k)

def _build_app_args(post_storage, notification_storage):
  return {
      'writer': {
        'stack_name': f"antipode-lambda-{post_storage}-{notification_storage}-writer",
        'lambda_name': f"AntipodeLambda{post_storage.title()}{notification_storage.title()}Writer",
      },
      'reader': {
        'stack_name': f"antipode-lambda-{post_storage}-{notification_storage}-reader",
        'lambda_name': f"AntipodeLambda{post_storage.title()}{notification_storage.title()}Reader",
      },
      'vpc_required': (CONNECTION_INFO['datastores'][post_storage]['vpc_required'] or CONNECTION_INFO['datastores'][notification_storage]['vpc_required']),
    }

def _template_env_variables():
  env = {}

  # transform all datastore info to env var style
  for storage,details in CONNECTION_INFO['datastores'].items():
    for k,v in details.items():
      # replace hyphens due to AWS limitation
      env[f"{storage}_{k.replace('-','_')}".upper()] = v

  # Add SQS eval queue info as well
  for k,v in CONNECTION_INFO['sqs_eval'].items():
    # replace hyphens due to AWS limitation
    env[f"SQS_EVAL_{k.replace('-','_')}".upper()] = v

  return env

def _is_inside_docker():
  return os.path.isfile('/.dockerenv')

def _force_docker():
  if _is_inside_docker():
    return

  import platform
  import subprocess
  from plumbum.cmd import docker

  os.chdir(ROOT_PATH)

  # if image is not built, we do it
  if bool(int(os.environ.get('REBUILD_DOCKER_IMAGE',0))) or docker['images', 'antipode-lambda:latest', '--format', '"{{.ID}}"']().strip() == '':
    docker['build', '--no-cache', '-t', 'antipode-lambda:latest', '.'] & FG

  args = list()
  args.extend(['docker', 'run', '--rm', '-it',
    # mount code volumes
    '-v', f"{ROOT_PATH}:/app",
    'antipode-lambda:latest'
  ])
  # force first argument to be a relative path -- important for plumbum local exec
  sys.argv[0] = './antipode_lambda'
  # merge docker args with original args
  args = args + sys.argv
  # print(' '.join(args))
  subprocess.call(args)
  exit()

#--------------
# BUILD
#--------------
def _build_service(role, args):
  from jinja2 import Environment
  import textwrap
  from plumbum.cmd import sam
  import boto3
  import shutil

  app_args = _build_app_args(args['post_storage'], args['notification_storage'])
  lambda_name = app_args[role]['lambda_name']
  deploy_dir = ROOT_PATH / 'deploy' / role

  # save build variables for other commands
  _put_last(f"{role}_stack_name", app_args[role]['stack_name'])

  print(f"\t[INFO] Building {role} application... ", flush=True)
  # remove old building dirs
  shutil.rmtree(deploy_dir, ignore_errors=True)
  # create empty new ones
  os.makedirs(deploy_dir, exist_ok=True)
  os.chdir(deploy_dir)

  template = """
    AWSTemplateFormatVersion: '2010-09-09'
    Transform: 'AWS::Serverless-2016-10-31'
    Description: 'Antipode Lambda - {{ post_storage }}<>{{ notification_storage }} - {{ role }}'
    Resources:
      {{ lambda_name }}:
        Type: AWS::Serverless::Function
        Properties:
          Handler: {{ role }}.lambda_handler
          Runtime: python3.7
          Description: 'Antipode Lambda - {{ post_storage }}<>{{ notification_storage }} - {{ role }}'
          MemorySize: 128
          Timeout: {{ lambda_timeout }}
          Role: {{ iam_lambda_admin_role_arn }}
          {% if vpc_required %}
          VpcConfig:
            SubnetIds:
              - {{ vpc_subnet_id }}
            SecurityGroupIds:
              - {{ vpc_security_group_id }}
          {% endif %}
          {% if role == 'reader' %}{{ reader_event_source | indent(10, False) }}{% endif %}
          Environment:
            Variables:
              POST_STORAGE: {{ post_storage }}
              NOTIFICATION_STORAGE: {{ notification_storage }}
              WRITER_REGION: {{ writer_region }}
              READER_REGION: {{ reader_region }}
              ANTIPODE: {{ antipode_enabled }}
              ANTIPODE_RENDEZVOUS_ENABLED: {{ antipode_rendezvous_enabled }}
              RENDEZVOUS: {{ rendezvous_enabled }}
              RENDEZVOUS_EU_CENTRAL_1: {{ rendezvous_eu }}
              RENDEZVOUS_US_EAST_1: {{ rendezvous_us }}
              RENDEZVOUS_AP_SOUTHEAST_1: {{ rendezvous_ap }}
              DELAY_MS: {{ delay_ms }}
              #---
              {% for k,v in env_variables.items() %}{{ k }}: {{ v }}
              {% endfor %}
  """
  template_generated = Environment().from_string(template).render({
    'role': role,
    'lambda_name': lambda_name,
    'lambda_timeout': LAMBDA_TIMEOUT_SECONDS,
    'iam_lambda_admin_role_arn': CONNECTION_INFO['iam_lambda_admin_role_arn'],
    'post_storage': args['post_storage'],
    'notification_storage': args['notification_storage'],
    'writer_region': args['writer'],
    'reader_region': args['reader'],
    'antipode_enabled': int(args['antipode']),
    'rendezvous_enabled': int(args['rendezvous']),
    'antipode_rendezvous_enabled': int(args['antipode_rendezvous']),
    'rendezvous_eu': CONNECTION_INFO['rendezvous']['eu-central-1'],
    'rendezvous_us': CONNECTION_INFO['rendezvous']['us-east-1'],
    'rendezvous_ap': CONNECTION_INFO['rendezvous']['ap-southeast-1'],
    'delay_ms': args['delay'],
    # vpc configurations if needed
    'vpc_required': app_args['vpc_required'],
    'vpc_subnet_id': CONNECTION_INFO['lambda']['network'][args[role]]['subnet_id'],
    'vpc_security_group_id': CONNECTION_INFO['lambda']['network'][args[role]]['security_group_id'],
    # each notification storage has a different event source for the reader lambda
    'reader_event_source': getattr(sys.modules[__name__], f"reader_event_source__{args['notification_storage']}")(),
    # we pass everything and leave for lambda implementation to decide what and how to use
    'env_variables': _template_env_variables(),
  })
  with open('template.yaml', 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_generated).strip())

  # copy datastore lib files
  shutil.copy(ROOT_PATH / 'lambdas' / f"{role}.py", deploy_dir)
  shutil.copy(ROOT_PATH / 'lambdas' / args['post_storage'] / f"{args['post_storage']}.py", deploy_dir)
  shutil.copy(ROOT_PATH / 'lambdas' / args['notification_storage'] / f"{args['notification_storage']}.py", deploy_dir)

  # merge requirements files from all libs into one
  requirements_files = [
    ROOT_PATH / 'lambdas' / 'requirements.txt',
    ROOT_PATH / 'lambdas' / args['post_storage'] / 'requirements.txt',
    ROOT_PATH / 'lambdas' / args['notification_storage'] / 'requirements.txt',
  ]
  all_requirements = []
  for fname in requirements_files:
    with open(fname) as infile:
      all_requirements.append(infile.read())
  with open(deploy_dir / 'requirements.txt', 'w') as outfile:
    outfile.write('\n'.join(set(all_requirements)))

  # copy antipode code if its enabled
  if args['antipode']:
    shutil.copy(ROOT_PATH / 'lambdas' / 'antipode' / 'antipode.py', deploy_dir)
    shutil.copy(ROOT_PATH / 'lambdas' / 'antipode' / f"antipode_{args['post_storage']}.py", deploy_dir)

  # copy rendezvous if its enabled
  if args['rendezvous']:
    shutil.copy(ROOT_PATH / 'lambdas' / 'rendezvous' / f"rendezvous_{args['post_storage']}.py", deploy_dir)
    shutil.copy(ROOT_PATH / 'lambdas' / 'rendezvous' / 'rendezvous_shim.py', deploy_dir)
    shutil.copy(ROOT_PATH / 'lambdas' / 'rendezvous' / 'rendezvous.py', deploy_dir)
    # copy auto-generated protobuf files 
    for filename in ['rendezvous_pb2_grpc.py', 'rendezvous_pb2.py', 'rendezvous_pb2.pyi']:
      shutil.copy(ROOT_PATH / 'lambdas' / 'rendezvous' / 'proto' / filename, deploy_dir)

  # figure out deploy params
  region = args[role]
  s3_bucket = CONNECTION_INFO['lambda']['s3_buckets'][region]
  stack_name = app_args[role]['stack_name']
  lambda_name = app_args[role]['lambda_name']

  # Due to bug with SSL we comment this
  # sam['validate', '--region', region] & FG

  # If any bug happens try this flag: --use-container
  # originally found due to this bug: https://github.com/aws/aws-lambda-builders/issues/229
  sam['build'] & FG
  sam['deploy',
      '--region', region,
      '--stack-name', stack_name,
      '--s3-bucket', s3_bucket,
      '--s3-prefix', stack_name,
      '--force-upload',
      '--profile', CONNECTION_INFO['aws_credentials_profile'],
      '--role-arn', CONNECTION_INFO['iam_cloudformation_admin_role_arn'],
      '--capabilities', 'CAPABILITY_IAM',
      '--no-confirm-changeset',
      '--no-fail-on-empty-changeset' # so if no changes this does not exit
    ] & FG
  # grab the arn of the deployed lambda
  cli_cloudformation = boto3.client('cloudformation', region_name=region)
  stack_deployed_lambda = cli_cloudformation.describe_stack_resource(StackName=stack_name, LogicalResourceId=lambda_name)['StackResourceDetail']
  cli_lambda = boto3.client('lambda', region_name=region)
  lambda_details = cli_lambda.get_function(FunctionName=stack_deployed_lambda['PhysicalResourceId'])
  _put_last(f"{role}__lambda__arn", lambda_details['Configuration']['FunctionArn'])

  os.chdir(ROOT_PATH)

def _compile_rendezvous_protos():
    import subprocess
    
    proto_path = 'lambdas/rendezvous/proto'
    command = f"""
        python3 -m grpc_tools.protoc \
        -I {proto_path} \
        --python_out={proto_path} \
        --pyi_out={proto_path} \
        --grpc_python_out={proto_path} \
        {proto_path}/rendezvous.proto \
    """
    result = subprocess.run(command, shell=True, capture_output=True, text=True)

    if result.returncode != 0:
      print(f'[ERROR] Could not compile rendezvous protobuf files {result.stderr}')
      exit(-1)

def build(args):
  _force_docker()

  # enable antipode in both scenarios
  if args['antipode_rendezvous']:
    args['antipode'] = True
  # save build variables for other commands
  _put_last('antipode_enabled', args['antipode'])
  _put_last('rendezvous_enabled', args['rendezvous'])
  _put_last('delay', args['delay'])
  _put_last('post_storage', args['post_storage'])
  _put_last('notification_storage', args['notification_storage'])
  # replace macro region with specific region
  # if we are only building one of the roles we get the last deployed role from file
  for role in ['reader', 'writer']:
    if not args[role]:
      args[role] = _get_last(f"{role}_region")
    else:
      args[role] = REGIONS[args[role]]
      _put_last(f"{role}_region", args[role])

  # compile rendezvous proto files
  if args['rendezvous']:
    print(f"[INFO] Compiling rendezvous protobuf files... ", flush=True)
    _compile_rendezvous_protos()

  # trigger build for writer and reader
  print(f"[INFO] Building {args['post_storage']}-{args['notification_storage']} application... ", flush=True)
  _build_service('writer', args)
  _build_service('reader', args)
  print("Done!")

def reader_event_source__sns():
  from jinja2 import Environment
  import textwrap

  # SNS doesnt have global replication hence the reader gets the notifications straight from the writer region topic
  # an alternative would be more complex: create a queue to send messagest from SNS to SQS, cross-region subscribe and read in the lambda
  writer_region = _get_last(f"writer_region")

  template = """
    Events:
      SnsEvent:
        Type: SNS
        Properties:
          Region: {{ writer_region }}
          Topic: {{ sns_arn }}
  """
  template_generated = Environment().from_string(template).render({
    'writer_region': writer_region,
    'sns_arn': CONNECTION_INFO['datastores']['sns'][f"arn__{writer_region}__writer"],
  })
  return textwrap.dedent(template_generated).strip()

def reader_event_source__dynamo():
  from jinja2 import Environment
  import textwrap

  # each region has a different stream for the notification table
  reader_region = _get_last(f"reader_region")
  stream_arn = CONNECTION_INFO['datastores']['dynamo'][f"notifications_table_stream_arn__{reader_region}"]

  template = """
    Events:
      DynamoDbEvent:
        Type: DynamoDB
        Properties:
          BatchSize: 1
          StartingPosition: LATEST
          Stream: {{ notifications_table_stream_arn }}
  """
  template_generated = Environment().from_string(template).render({
    'notifications_table_stream_arn': stream_arn,
  })

  return textwrap.dedent(template_generated).strip()

def reader_event_source__mq():
  from jinja2 import Environment
  import textwrap

  # each region has a different stream for the notification table
  reader_region = _get_last(f"reader_region")
  broker_arn = CONNECTION_INFO['datastores']['mq'][f"arn__{reader_region}"]
  queue = CONNECTION_INFO['datastores']['mq']['notification_queue']
  secretmanager_arn = CONNECTION_INFO['datastores']['mq'][f"secretmanager_arn__{reader_region}"]

  template = """
    Events:
      MQEvent:
        Type: MQ
        Properties:
          Broker: {{ reader_broker_arn }}
          Queues:
            - {{ notifications_queue }}
          SourceAccessConfigurations:
            - Type: BASIC_AUTH
              URI: {{ secretmanager_arn }}
          BatchSize: 1
  """
  template_generated = Environment().from_string(template).render({
    'reader_broker_arn': broker_arn,
    'notifications_queue': queue,
    'secretmanager_arn': secretmanager_arn,
  })

  return textwrap.dedent(template_generated).strip()


#--------------
# CLEAN
#--------------
def _clean_sqs_eval():
  import boto3
  from botocore.exceptions import ClientError

  try:
    print("\t[INFO] Purging SQS eval queue... ", end='', flush=True)
    sqs = boto3.resource('sqs', region_name=_get_last('reader_region'))
    queue = sqs.get_queue_by_name(QueueName=CONNECTION_INFO['sqs_eval']['name'])
    queue.purge()
  except ClientError as e:
    if type(e).__name__ == 'PurgeQueueInProgress':
      print("Queue purge already in progress, try in 60seconds... ", end='', flush=True)
    else:
      raise

  print("Done!")

def _clean_service(role):
  from plumbum.cmd import aws

  print(f"\t[INFO] Deleting {role} stack... ", end='', flush=True)
  aws['cloudformation', 'delete-stack',
      '--region', _get_last(f"{role}_region"),
      '--stack-name', _get_last(f"{role}_stack_name"),
  ] & FG
  print("Done!")

def clean(args):
  _force_docker()

  # it actually means neither writer or reader got selected
  # hence we execute both
  if args['strong'] and (not args['writer'] and not args['reader']):
    args['writer'] = args['reader'] = True

  # if we passed writer or reader clean option that means its a strong clean for either
  if args['writer'] or args['reader']:
    args['strong'] = True

  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')

  print(f"[INFO] Cleaning '{post_storage}-{notification_storage}'... ")
  getattr(sys.modules[__name__], f"clean__{post_storage}")()
  getattr(sys.modules[__name__], f"clean__{notification_storage}")()
  _clean_sqs_eval()

  if args['strong']:
    if args['writer']:
      _clean_service('writer')

    if args['reader']:
      _clean_service('reader')

  # set flag that the experiment was cleaned
  _put_last('cleaned', True)
  print("[INFO] Done!")

def clean__mysql():
  import pymysql
  import pymysql.cursors

  MYSQL_CONN = CONNECTION_INFO['datastores']['mysql']

  # clean table before running lambda
  print("\t[INFO] Truncating MySQL table... ", end='', flush=True)
  mysql_conn = pymysql.connect(
      host=MYSQL_CONN[f"host__{_get_last('writer_region')}__writer"],
      port=MYSQL_CONN['port'],
      user=MYSQL_CONN['user'],
      password=MYSQL_CONN['password'],
      connect_timeout=60,
      autocommit=True
    )
  with mysql_conn.cursor() as cursor:
    try:
      sql = f"DROP DATABASE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
    except (pymysql.err.InternalError, pymysql.err.OperationalError) as e:
      code, msg = e.args
      if code == 1008:
        # database does not exist hence we ignore
        pass
      else:
        print(f"[WARN] MySQL error: {e}")
        exit(-1)

    try:
      sql = f"CREATE DATABASE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
      sql = f"USE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
    except pymysql.err.InternalError as e:
      print(f"[WARN] MySQL error: {e}")
      exit(-1)

    if _get_last('rendezvous_enabled'):
      sql = f"CREATE TABLE `{MYSQL_CONN['post_table_name']}` (k BIGINT, v VARCHAR({NOTIFICATION_KEY_LEN}), b LONGBLOB, rdv_bid VARCHAR(255))"
    else:
      sql = f"CREATE TABLE `{MYSQL_CONN['post_table_name']}` (k BIGINT, v VARCHAR({NOTIFICATION_KEY_LEN}), b LONGBLOB)"
    
    cursor.execute(sql)
    mysql_conn.commit()

    if _get_last('rendezvous_enabled'):
      sql = f"CREATE INDEX rdv_bid_index ON {MYSQL_CONN['post_table_name']} (rdv_bid)"
      cursor.execute(sql)
      mysql_conn.commit()

    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['post_table_name']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    sql = f"CREATE TABLE `{MYSQL_CONN['notifications_table_name']}` (k BIGINT, v VARCHAR({NOTIFICATION_KEY_LEN}))"
    cursor.execute(sql)
    mysql_conn.commit()
    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['notifications_table_name']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    sql = f"CREATE TABLE `{MYSQL_CONN['antipode_table']}` (cid VARCHAR(32), json VARCHAR(20000))"
    cursor.execute(sql)
    mysql_conn.commit()
    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['antipode_table']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    print("Done!")

def clean__sns():
  # no need to clean sns
  None

def clean__dynamo():
  import boto3

  print("\t[INFO] Truncating Dynamo table... ", end='', flush=True)
  DYNAMO_CONN = CONNECTION_INFO['datastores']['dynamo']
  for zone in [_get_last('writer_region'), _get_last('reader_region')]:
    resource_dynamo = boto3.resource('dynamodb', region_name=zone)
    table_names = [
      DYNAMO_CONN['post_table_name'],
      DYNAMO_CONN['notifications_table_name'],
      DYNAMO_CONN['antipode_table']
    ]

    if _get_last('rendezvous_enabled'):
      table_names.append(DYNAMO_CONN['rendezvous_table'])

    for table_name in table_names:
      table = resource_dynamo.Table(table_name)
      # Only way to clean a table in DynamoDB is to go key by key

      # get key attributes name
      table_key_names = [ key.get("AttributeName") for key in table.key_schema ]
      # only retrieve the keys for each item in the table (minimize data transfer)
      expression_projection = ", ".join('#' + key for key in table_key_names)
      expression_attributes = { '#'+key: key for key in table_key_names }
      # delete items per page
      page = table.scan(ProjectionExpression=expression_projection, ExpressionAttributeNames=expression_attributes)
      with table.batch_writer() as batch:
        while page['Count'] > 0:
          # Delete items in batches
          for item_keys in page['Items']:
            batch.delete_item(Key=item_keys)
          # Fetch the next page
          if 'LastEvaluatedKey' in page:
            time.sleep(4) # to avoid throttling
            page = table.scan(
                ProjectionExpression=expression_projection,
                ExpressionAttributeNames=expression_attributes,
                ExclusiveStartKey=page['LastEvaluatedKey']
              )
          else:
            break

      # assert that has 0 items
      # cli_dynamo = boto3.client('dynamodb', region_name=zone)
      # ItemCount is not updated frequently - impossible to assert
      # table_describe = cli_dynamo.describe_table(TableName=table_name)
      # assert(table_describe['Table']['ItemCount'] == 0)
  print("Done!")

def clean__s3():
  import boto3

  print("\t[INFO] Cleaning S3 buckets... ", end='', flush=True)
  s3_resource = boto3.resource('s3')
  S3_CONN = CONNECTION_INFO['datastores']['s3']

  for role in ['reader', 'writer']:
    region = _get_last(f"{role}_region")
    bucket = S3_CONN[f"bucket__{region}__{role}"]
    s3_resource.Bucket(bucket).objects.all().delete()

  print("Done!")

def clean__mq():
  # ref: https://alediaferia.com/2020/07/30/how-to-keep-your-amazon-mq-queues-clean/
  from jinja2 import Environment
  import textwrap
  import shutil
  from plumbum.cmd import amazonmq_cli

  print("\t[INFO] Purging MQ... ", end='', flush=True)

  # force docker if needed

  MQ_CONN = CONNECTION_INFO['datastores']['mq']

  # create config file for the brokers
  template = """
    broker {
      antipode-writer {
        web-console = "https://{{ writer_host }}:8162/admin/"
        amqurl = "ssl://{{ writer_host }}:61617"
        username = "{{ username }}"
        password = "{{ password }}"
        prompt-color = "light-blue"
      }
      antipode-reader {
        web-console = "https://{{ reader_host }}:8162/admin/"
        amqurl = "ssl://{{ reader_host }}:61617"
        username = "{{ username }}"
        password = "{{ password }}"
        prompt-color = "light-blue"
      }
    }
    web-console {
      pause = 100
      timeout = 300000
    }
  """
  template_generated = Environment().from_string(template).render({
    'writer_host': MQ_CONN[f"host__{_get_last('writer_region')}"],
    'reader_host': MQ_CONN[f"host__{_get_last('reader_region')}"],
    'username': MQ_CONN['user'],
    'password': MQ_CONN['password'],
  })
  with open(ROOT_PATH / 'deploy' / 'mq-brokers.config', 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_generated).strip())

  # save the commands to execute
  with open(ROOT_PATH / 'deploy' / 'mq-purge.cmd', 'w') as f:
    print("connect --broker antipode-writer", file=f)
    print("purge-all-queues --force", file=f)
    print("disconnect", file=f)
    print("connect --broker antipode-reader", file=f)
    print("purge-all-queues --force", file=f)

  # copy file to config
  shutil.copy(ROOT_PATH / 'deploy' / 'mq-brokers.config', '/tools/amazonmq-cli/conf/amazonmq-cli.config')

  # run command hidding the output
  (amazonmq_cli['--cmdfile', ROOT_PATH / 'deploy' / 'mq-purge.cmd'] & BG).wait()
  print("Done!")

def clean__cache():
  import boto3

  print("\t[INFO] Cleaning Redis ElastiCache... ", end='', flush=True)
  # only way to clean cache is from within the VPC
  # hence we trigger a Lambda with a specific clean action tag
  writer_region = _get_last(f"writer_region")
  writer_lambda_arn = _get_last(f"writer__lambda__arn")

  writer_client = boto3.client('lambda', region_name=writer_region)
  response = writer_client.invoke(
      FunctionName=writer_lambda_arn,
      InvocationType='RequestResponse',
      Payload= json.dumps({ LAMBDA_CLEAN_TAG: 1 })
    )
  # exists if we get a 200 or 202 (for async) return code
  status_code = response['ResponseMetadata']['HTTPStatusCode']
  if status_code != 200:
    raise Exception('Failed to clean cache storage')
  print("Done!")

#--------------
# RUN
#--------------
def _writer_invoke(args):
  import boto3

  mode, writer_region, writer_lambda_arn, latencies, i = args
  payload = _generate_payload(i)
  writer_client = boto3.client('lambda', region_name=writer_region)
  response = writer_client.invoke(
      FunctionName=writer_lambda_arn,
      InvocationType=mode,
      Payload=json.dumps(payload),
    )

  # save the time which we got a reply from
  latencies[i] = datetime.utcnow().timestamp()

  # exists if we get a 200 or 202 (for async) return code
  status_code = response['ResponseMetadata']['HTTPStatusCode']

  # replace status_code with the one frmo within the lambda
  if status_code == 200:
    status_code = int(json.loads(response['Payload'].read()).get('statusCode', 500))

  # now we parse the responses
  if status_code == 200 or status_code == 202:
    return
  else:
    latencies[i] = None
    print(f"[ERROR] Lambda {status_code} error, skipping...")

def _writer_pool_init():
  import signal
  # Ignore CTRL+C in the worker process
  signal.signal(signal.SIGINT, signal.SIG_IGN)

def run(args):
  from tqdm import tqdm
  import click

  # check if experiment was cleaned and if not we ask the user if he wants to continue
  if not _get_last('cleaned') and not click.confirm('[WARN] The experiment was not cleaned before hand! You might gather old results afterwards. Do you still want to continue?', default=False):
    exit(-1)

  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')
  writer_region = _get_last(f"writer_region")
  writer_lambda_arn = _get_last(f"writer__lambda__arn")
  mode = 'Event' if args['async'] else 'RequestResponse'

  l = dict()
  with mp.Manager() as manager:
    # dict for requests latencies
    latencies = manager.dict()
    # build args for the invoke -- KEEP THE ORDER OF THE ARGUMENTS or adjust in the invoke method
    invoke_args_range = [ (mode, writer_region, writer_lambda_arn, latencies, i) for i in range(args['requests']) ]

    print(f"[INFO] Running '{post_storage}-{notification_storage}' for #{args['requests']} ... ")
    # invoke writer in paralell
    pool = mp.Pool(processes=CORE_COUNT, initializer=_writer_pool_init)
    try:
      for _ in tqdm(pool.imap_unordered(_writer_invoke,  invoke_args_range), total=args['requests']):
        pass
      # remove errors from entries
      l = { k:v for k, v in dict(latencies).items() if v is not None }
    except KeyboardInterrupt:
      pool.terminate()
    finally:
      pool.close()
      pool.join()

  # save variables in last file
  print(f"[INFO] Saving {len(l)} requests from '{post_storage}-{notification_storage}'!")
  _put_last('latencies', l)
  _put_last('cleaned', False)
  _put_last('requests', len(l))
  print("Done!")


#--------------
# GATHER
#--------------
def gather(args):
  from tqdm import tqdm
  import pandas as pd
  import boto3

  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')
  antipode_enabled = _get_last('antipode_enabled')
  rendezvous_enabled = _get_last('rendezvous_enabled')
  num_requests = _get_last('requests')
  delay = _get_last('delay')
  latencies = _get_last('latencies')
  reader_region = _get_last('reader_region')
  writer_region = _get_last('writer_region')

  queue_url = CONNECTION_INFO['sqs_eval'][f"url__{reader_region}"]
  queue_url = CONNECTION_INFO['sqs_eval'][f"url__{reader_region}"]

  cli_sqs = boto3.client('sqs', region_name=reader_region)
  if num_requests != len(latencies):
    print("[ERROR] Mismatch between number of requests and number of available latencies!")
    exit(-1)

  print("[INFO] Waiting for all messages to arrive at eval queue... ", flush=True)
  num_messages = 0
  try:
    with tqdm(total=num_requests) as pbar:
      while True:
        reply = cli_sqs.get_queue_attributes(
            QueueUrl=queue_url,
            AttributeNames=[ 'ApproximateNumberOfMessages' ]
          )
        num_messages = int(reply['Attributes']['ApproximateNumberOfMessages'])
        # update pbar
        pbar.n = min(num_messages, num_requests)
        pbar.refresh()
        # break if necessary or sleep 1 second per message missing
        if num_messages >= num_requests:
          break
        else:
          time.sleep((num_requests-num_messages)/100.0)
    print("Done!")
  except KeyboardInterrupt:
    print(f"[WARN] Skipping missing results {num_messages}/{num_requests}")
    # set new number of requests to messages
    num_requests = num_messages

  print("[INFO] Gather info from SQS... ", flush=True)
  # gather results - ideally all but if we cancel tqdm we move on with the ones we have
  results = {}
  try:
    with tqdm(total=num_requests) as pbar:
      while len(results) < num_requests:
        # We noticed expired auth problems when this loop takes longer
        # hence we move this into the loop - gather can be slow anyways
        resource_sqs = boto3.resource('sqs', region_name=reader_region)
        eval_queue = resource_sqs.get_queue_by_name(QueueName=CONNECTION_INFO['sqs_eval']['name'])

        # fetch max 10 messages :(
        messages = eval_queue.receive_messages(MaxNumberOfMessages=10)
        # sleep if no messages returned from SQS
        if len(messages) == 0:
          time.sleep(10)
          continue

        for message in messages:
          # events might come from Lambda destinations (with responsePayload)
          # or sent manually inside lambdas
          item = json.loads(message.body)
          if 'responsePayload' in item:
            item = json.loads(item['responsePayload']['body'])

          # index by i so we avoid duplicate results
          e = {
            # client
            'client_sent_at': float(item['client_sent_at']),
            'reply_at': float(latencies[int(item['i'])]),
            # writer
            'writer_start_at': float(item['writer_start_at']),
            # reader
            'reader_received_at': float(item['reader_received_at']),
            'post_read_at': float(item['post_read_at']),
            'consistent_read': int(item['consistent_read']), # 0 - False, 1 - True
            'notification_to_reader_spent_ms': int(item['notification_to_reader_spent_ms']),
            'post_written_at': int(item['post_written_at']),
            'write_post_spent_ms': int((item['post_written_at']- item['writer_start_at']) * 1000),
            'read_post_spent_ms': int(item['read_post_spent_ms'])
          }
          if antipode_enabled:
            e['antipode_spent_ms'] = int(item['antipode_spent_ms'])

          if rendezvous_enabled:
            #e['rendezvous_writer_spent_ms'] = int(item['rendezvous_writer_spent_ms'])
            #e['rendezvous_reader_spent_ms'] = int(item['rendezvous_reader_spent_ms'])
            e['rendezvous_call_writer_spent_ms'] = int(item['rendezvous_call_writer_spent_ms'])
            e['rendezvous_call_reader_spent_ms'] = int(item['rendezvous_call_reader_spent_ms'])
            e['rendezvous_prevented_inconsistency'] = int(item['rendezvous_prevented_inconsistency'])

          results[int(item['i'])] = e
          message.delete()
          pbar.update(1)
    print("[INFO] Done!")
  except KeyboardInterrupt:
    print(f"[WARN] Skipping remaining results {len(results)}/{num_requests}")

  # tag for this gather
  if args['tag'] is None:
    args['tag'] = f"{_dict_reverse_lookup(REGIONS, writer_region)}-{_dict_reverse_lookup(REGIONS, reader_region)}__{num_requests}__delay-{delay}ms"
    if antipode_enabled:
      args['tag'] = 'antipode__' + args['tag']
    if rendezvous_enabled:
      args['tag'] = 'rendezvous__' + args['tag']

  # now we compute date over the global results and tidy up dict for dataframe
  eval_values = results.values()
  first_client_sent = min(eval_values, key=lambda k: k['client_sent_at'])['client_sent_at']
  last_client_replied = max(eval_values, key=lambda k: k['reply_at'])['reply_at']

  # for each entry in dict we compute results and delete keys no longer needed
  for _,r in results.items():
    # visibility latency from the client
    r['client_latency_ms'] = int((r['reply_at'] - r['client_sent_at']) * 1000)
    r['client_visibility_latency_ms'] = int((r['post_read_at'] - r['client_sent_at']) * 1000)
    # visibility latency from the proxy
    r['writer_latency_ms'] = int((r['reply_at'] - r['writer_start_at']) * 1000)
    r['writer_visibility_latency_ms'] = int((r['post_read_at'] - r['writer_start_at']) * 1000)

    # delete no longer needed keys
    del r['client_sent_at']
    del r['writer_start_at']
    del r['reply_at']
    del r['post_read_at']

  print("[INFO] Parsing evaluation ...", end='')
  df = pd.DataFrame(results.values())

  # generate path to save the results
  gather_path = ROOT_PATH / 'gather' / f"{post_storage}-{notification_storage}"  / f"{args['tag']}-{time.strftime('%Y%m%d%H%M%S')}"
  os.makedirs(gather_path, mode=0o777, exist_ok=True)
  csv_path = gather_path / 'traces.csv'
  info_path = gather_path / 'traces.info'

  # save to csv so we can plot a timeline later
  df.to_csv(csv_path, sep=';', mode='w')
  print(f"[INFO] Save '{csv_path}'")

  # figure out the throughput
  num_results = len(results)
  # from sent_at@i=0 to sent_at@i=0
  time_spent_by_client = last_client_replied - first_client_sent

  # values that we will write in the info file as tags for later parsing
  eval_tags = {
      'NUM_RESULTS': f"{num_results}/{_get_last('requests')}",
      '%_INCONSISTENCIES': len(df.query('consistent_read == 0')) / float(len(df)),
      'THROUGHPUT_ON_CLIENT': num_results / float(time_spent_by_client),
    }
  
  if rendezvous_enabled:
    eval_tags['%_PREVENTED_INCONSISTENCIES'] = len(df.query('rendezvous_prevented_inconsistency == 1')) / float(len(df))

  # save the pandas describe
  # pd.set_option('display.float_format', lambda x: '%.3f' % x)
  pd.set_option('display.html.table_schema', True)
  pd.set_option('display.precision', 2)
  pd.set_option('display.max_columns', None)
  pd.set_option('display.expand_frame_repr', False)
  with open(info_path, 'w') as f:
    for tag,v in eval_tags.items():
      print(f"[{tag}] {v}", file=f)
    print("", file=f)
    print(df.describe(percentiles=PERCENTILES_TO_PRINT), file=f)
  print(f"[INFO] Save '{info_path}'\n")
  # print file to stdout
  with open(info_path, 'r') as f:
    print(f.read())


#--------------
# CONSTANTS
#--------------
ROOT_PATH = Path(os.path.abspath(os.path.dirname(sys.argv[0])))
LAST_INFO_FILE = ROOT_PATH / '.last.yml'
#--------------
# post_storage METHODS:
#   - write_post(i, k) -> op
#   - read_post(k, evaluation)
#   - antipode_shim(id, role) -> AntipodeBridge
#--------------
# notification_storage METHODS:
#   - write_notification(event)
#   - parse_event(event)
#--------------
POST_STORAGE = [
  'mysql',
  'dynamo',
  's3',
  'cache',
]
NOTIFICATION_STORAGE = [
  'sns',
  'dynamo',
  'mq',
]
CONNECTION_INFO = _load_yaml(ROOT_PATH / 'connection_info.yaml')
REGIONS = {
  'eu': 'eu-central-1',
  'us': 'us-east-1',
  'sg': 'ap-southeast-1',
}
LAMBDA_TIMEOUT_SECONDS = 900
LAMBDA_CLEAN_TAG = "-#CLEAN#-"
# cap CPU cores are 8
CORE_COUNT = min(mp.cpu_count(), 8)
NOTIFICATION_KEY_LEN = 10
PERCENTILES_TO_PRINT = [.25, .5, .75, .90, .99]

#--------------
# CMD LINE
#--------------
if __name__ == '__main__':
  # parse arguments
  main_parser = argparse.ArgumentParser()

  # different commands
  subparsers = main_parser.add_subparsers(help='commands', dest='which')

  # build application
  build_parser = subparsers.add_parser('build', help='Build application')
  build_parser.add_argument('-ps', '--post-storage', required=True, choices=POST_STORAGE, help="Post Storage datastore")
  build_parser.add_argument('-ns', '--notification-storage', required=True, choices=NOTIFICATION_STORAGE, help="Notification Storage datastore")
  build_parser.add_argument('-w', '--writer', required=True, choices=REGIONS.keys(), help="Build writer on the specified region")
  build_parser.add_argument('-r', '--reader', required=True, choices=REGIONS.keys(), help="Build reader on the specified region")
  build_parser.add_argument('-ant', '--antipode', action='store_true', help="Enables antipode on the lambdas")
  build_parser.add_argument('-rdv', '--rendezvous', action='store_true', help="Enables rendezvous on the lambdas")
  build_parser.add_argument('-ant-r', '--antipode-rendezvous', action='store_true', help="Enables antipode rendezvouz point version")
  build_parser.add_argument('-d', '--delay', type=int, default=0, help="Apply ms delay before writing notification")

  # run application
  run_parser = subparsers.add_parser('run', help='Run application')
  run_parser.add_argument('-r', '--requests', type=int, default=1, help="Number of requests to run")
  # ref: https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html
  run_parser.add_argument('-a', '--async', action='store_true', help="Switches lambda invocation to async")

  # clean application
  clean_parser = subparsers.add_parser('clean', help='Clean application')
  clean_parser.add_argument('-s', '--strong', action='store_true', help="Delete SAM stacks & Lambdas")
  clean_parser.add_argument('-w', '--writer', action='store_true', help="Delete only the writer")
  clean_parser.add_argument('-r', '--reader', action='store_true', help="Delete only the reader")

  # run application
  gather_parser = subparsers.add_parser('gather', help='Gather eval')
  gather_parser.add_argument('-t', '--tag', type=str, default=None, help="Tags the gather")

  # parse args
  args = vars(main_parser.parse_args())
  command = args.pop('which')

  if command == 'run':
    # check if the number of requests is bigger than the range of notification keys available
    if len(str(args['requests'])) > NOTIFICATION_KEY_LEN:
      main_parser.error(f"Maximum allowed key length of {NOTIFICATION_KEY_LEN} - {len(str(args['requests']))} given. Please edit this value.")

  if command == 'build':
    # check if the number of requests is bigger than the range of notification keys available
    if args['delay'] > (LAMBDA_TIMEOUT_SECONDS * 1000.0):
      main_parser.error(f"Delay value bigger than lambda timeout {LAMBDA_TIMEOUT_SECONDS}s.")

  # call parser method dynamically
  getattr(sys.modules[__name__], command)(args)