#!/usr/bin/env python3

from pathlib import Path
from pprint import pprint
import os
import shutil
import sys
import argparse
import random
import string
import multiprocessing as mp
import boto3
import json
from datetime import datetime
from plumbum import local
from plumbum import FG, BG
import time
from tqdm import tqdm
import click
import pandas as pd
from pathlib import Path
from shutil import copyfile

# pd.set_option('display.float_format', lambda x: '%.3f' % x)
pd.set_option('display.html.table_schema', True)
pd.set_option('display.precision', 2)
pd.set_option('max_columns', None)
pd.set_option('display.expand_frame_repr', False)


#--------------
# AWS AURORA GLOBAL CLUSTER
#--------------
#   1. Go to eu-central-1 zone
#   2. Go to RDS dashboard and click on "Create Database"
#   3. Select "Standard Create"
#       - Engine type: Amazon Aurora
#       - MySQL compatibility
#       - Provisioned
#       - Single Master
#       - Select a version that supports "Global Database" feature
#       - Select PRODUCTION template
#       - Cluster name: 'antipode-lambda-eu'
#       - Username: 'antipode' / Password: 'antipode'
#       - Choose an instance class (tick "Include previous generations" for older and cheaper instances)
#       - Do not create Multi-AZ deployment
#       - Public access: YES
#       - Choose 'allow-all' VPC group
#       - Database port: 3306
#       - Disable Encryption
#       - Disable Performance Insights
#       - Disable Enhanced monitoring
#       - Disable auto minor version upgrade
#       - Disable delete protection
#   3. Wait for all the instances to be created
#   4. Select the 'antipode-lambda-eu' cluster and perform the action 'Add AWS zone'
#       - Global database identifier: antipode-lambda
#       - Secondary region: US East (N Virginia)
#       - *Choose all the above configurations for the new instance when possible*
#       - DB instance identifier: antipode-lambda-us-instance
#       - DB cluster identifier: antipode-lambda-us
#   . Modify 'antipode-lambda-us-instance-1' name to 'antipode-lambda-us-instance'
#
# ref: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html
#--------------

#--------------
# AWS SQS EVAL QUEUE
#--------------
#   1. Go to us-east-1 zone and to the AWS SQS dashboard
#   2. Create queue with the following parameters:
#         - Standard type
#         - name: *SQS_EVAL_QUEUE['name']*
#--------------

#--------------
# AWS SNS GLOBAL
#--------------
#   1. Go to eu-central-1 zone and to the AWS SNS dashboard
#   2. Go to Topics and create a new one with the following parameters:
#         - Standard type
#         - name: antipode-lambda-notifications
#--------------


#--------------
# HELPERS
#--------------
def _generate_payload(i):
  key = str(i) + ''.join(random.choices(string.ascii_uppercase, k=1)) + ''.join(random.choices(string.ascii_uppercase + string.digits, k=NOTIFICATION_KEY_LEN - len(str(i)) - 1))
  return { "i": i, "key": key, "timestamp": datetime.utcnow().timestamp() }

def _load_yaml(path):
  import yaml
  with open(path, 'r') as f:
    return yaml.safe_load(f)

def _put_last(k,v):
  import yaml
  # if file exists parse yaml otherwise create empty dict to write to
  if Path(LAST_INFO_FILE).exists():
    doc = _load_yaml(LAST_INFO_FILE)
  else:
    doc = {}
  # write new value and save to file
  doc[k] = v
  with open(LAST_INFO_FILE, 'w+') as f:
      yaml.safe_dump(doc, f, default_flow_style=False)

def _get_last(k):
  import yaml
  doc = _load_yaml(LAST_INFO_FILE)
  return doc.get(k)

def _build_app_args(post_storage, notification_storage):
  return {
    'writer': {
      'stack_name': f"antipode-lambda-{post_storage}-{notification_storage}-writer",
      'lambda_name': f"AntipodeLambda{post_storage.title()}{notification_storage.title()}Writer",
    },
    'reader': {
      'stack_name': f"antipode-lambda-{post_storage}-{notification_storage}-reader",
      'lambda_name': f"AntipodeLambda{post_storage.title()}{notification_storage.title()}Reader",
    }
  }

def _template_env_variables():
  env = {}

  # transform all datastore info to env var style
  for storage,details in CONNECTION_INFO['datastores'].items():
    for k,v in details.items():
      # replace hyphens due to AWS limitation
      env[f"{storage}_{k.replace('-','_')}".upper()] = v

  # Add SQS eval queue info as well
  for k,v in CONNECTION_INFO['sqs_eval'].items():
    # replace hyphens due to AWS limitation
    env[f"SQS_EVAL_{k.replace('-','_')}".upper()] = v

  return env


#--------------
# BUILD
#--------------
def _build_service(role, args):
  from jinja2 import Environment
  import textwrap
  from plumbum.cmd import sam

  app_args = _build_app_args(args['post_storage'], args['notification_storage'])
  lambda_name = app_args[role]['lambda_name']
  deploy_dir = ROOT_PATH / 'deploy' / role

  # save build variables for other commands
  _put_last(f"{role}_stack_name", app_args[role]['stack_name'])

  print(f"\t[INFO] Building {role} application... ", flush=True)
  # remove old building dirs
  shutil.rmtree(deploy_dir, ignore_errors=True)
  # create empty new ones
  os.makedirs(deploy_dir, exist_ok=True)
  os.chdir(deploy_dir)

  template = """
    AWSTemplateFormatVersion: '2010-09-09'
    Transform: 'AWS::Serverless-2016-10-31'
    Description: 'Antipode Lambda - {{ post_storage }}<>{{ notification_storage }} - {{ role }}'
    Resources:
      {{ lambda_name }}:
        Type: AWS::Serverless::Function
        Properties:
          Handler: {{ role }}.lambda_handler
          Runtime: python3.7
          Description: 'Antipode Lambda - {{ post_storage }}<>{{ notification_storage }} - {{ role }}'
          MemorySize: 128
          Timeout: 900
          Role: arn:aws:iam::641424397462:role/antipode-lambda-admin
          {% if role == 'reader' %}{{ reader_event_source | indent(10, False) }}{% endif %}
          Environment:
            Variables:
              POST_STORAGE: {{ post_storage }}
              NOTIFICATION_STORAGE: {{ notification_storage }}
              WRITER_REGION: {{ writer_region }}
              READER_REGION: {{ reader_region }}
              ANTIPODE: {{ antipode_enabled }}
              #---
              {% for k,v in env_variables.items() %}{{ k }}: {{ v }}
              {% endfor %}
  """
  template_generated = Environment().from_string(template).render({
    'role': role,
    'lambda_name': lambda_name,
    'post_storage': args['post_storage'],
    'notification_storage': args['notification_storage'],
    'writer_region': args['writer'],
    'reader_region': args['reader'],
    'antipode_enabled': int(args['antipode']),
    # each notification storage has a different event source for the reader lambda
    'reader_event_source': getattr(sys.modules[__name__], f"reader_event_source__{args['notification_storage']}")(),
    # we pass everything and leave for lambda implementation to decide what and how to use
    'env_variables': _template_env_variables(),
  })
  with open('template.yaml', 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_generated).strip())

  # copy datastore lib files
  shutil.copy(ROOT_PATH / 'lambdas' / f"{role}.py", deploy_dir)
  shutil.copy(ROOT_PATH / 'lambdas' / args['post_storage'] / f"{args['post_storage']}.py", deploy_dir)
  shutil.copy(ROOT_PATH / 'lambdas' / args['notification_storage'] / f"{args['notification_storage']}.py", deploy_dir)

  # merge requirements files from all libs into one
  requirements_files = [
    ROOT_PATH / 'lambdas' / 'requirements.txt',
    ROOT_PATH / 'lambdas' / args['post_storage'] / 'requirements.txt',
    ROOT_PATH / 'lambdas' / args['notification_storage'] / 'requirements.txt',
  ]
  all_requirements = []
  for fname in requirements_files:
    with open(fname) as infile:
      all_requirements.append(infile.read())
  with open(deploy_dir / 'requirements.txt', 'w') as outfile:
    outfile.write('\n'.join(set(all_requirements)))

  # figure out deploy params
  region = args[role]
  s3_bucket = CONNECTION_INFO['lambda_s3_buckets'][region]
  stack_name = app_args[role]['stack_name']
  lambda_name = app_args[role]['lambda_name']

  # Due to bug with SSL we comment this
  # sam['validate', '--region', region] & FG
  sam['build'] & FG
  sam['deploy',
      '--region', region,
      '--stack-name', stack_name,
      '--s3-bucket', s3_bucket,
      '--s3-prefix', stack_name,
      '--force-upload',
      '--profile', CONNECTION_INFO['aws_credentials_profile'],
      '--role-arn', CONNECTION_INFO['iam_cloudformation_admin_role_arn'],
      '--capabilities', 'CAPABILITY_IAM',
      '--no-confirm-changeset',
      '--no-fail-on-empty-changeset' # so if no changes this does not exit
    ] & FG
  # grab the arn of the deployed lambda
  cli_cloudformation = boto3.client('cloudformation', region_name=region)
  stack_deployed_lambda = cli_cloudformation.describe_stack_resource(StackName=stack_name, LogicalResourceId=lambda_name)['StackResourceDetail']
  cli_lambda = boto3.client('lambda', region_name=region)
  lambda_details = cli_lambda.get_function(FunctionName=stack_deployed_lambda['PhysicalResourceId'])
  _put_last(f"{role}__lambda__arn", lambda_details['Configuration']['FunctionArn'])

  os.chdir(ROOT_PATH)

def build(args):
  # save build variables for other commands
  _put_last('antipode_enabled', args['antipode'])
  _put_last('post_storage', args['post_storage'])
  _put_last('notification_storage', args['notification_storage'])
  # replace macro region with specific region
  # if we are only building one of the roles we get the last deployed role from file
  for role in ['reader', 'writer']:
    if not args[role]:
      args[role] = _get_last(f"{role}_region")
    else:
      args[role] = REGIONS[args[role]]
      _put_last(f"{role}_region", args[role])

  # trigger build for writer and reader
  print(f"[INFO] Building {args['post_storage']}-{args['notification_storage']} application... ", flush=True)
  _build_service('writer', args)
  _build_service('reader', args)
  print("Done!")

def reader_event_source__sns():
  from jinja2 import Environment
  import textwrap

  # SNS doesnt have global replication hence the reader gets the notifications straight from the writer region topic
  # an alternative would be more complex: create a queue to send messagest from SNS to SQS, cross-region subscribe and read in the lambda
  writer_region = _get_last(f"writer_region")

  template = """
    Events:
      SnsEvent:
        Type: SNS
        Properties:
          Region: {{ writer_region }}
          Topic: {{ sns_arn }}
  """
  template_generated = Environment().from_string(template).render({
    'writer_region': writer_region,
    'sns_arn': CONNECTION_INFO['datastores']['sns'][f"arn__{writer_region}__writer"],
  })
  return textwrap.dedent(template_generated).strip()

def reader_event_source__dynamo():
  from jinja2 import Environment
  import textwrap

  # each region has a different stream for the notification table
  reader_region = _get_last(f"reader_region")
  stream_arn = CONNECTION_INFO['datastores']['dynamo'][f"notifications_table_stream_arn__{reader_region}"]

  template = """
    Events:
      DynamoDbEvent:
        Type: DynamoDB
        Properties:
          BatchSize: 1
          StartingPosition: LATEST
          Stream: {{ notifications_table_stream_arn }}
  """
  template_generated = Environment().from_string(template).render({
    'notifications_table_stream_arn': stream_arn,
  })

  return textwrap.dedent(template_generated).strip()

#--------------
# CLEAN
#--------------
def _clean_sqs_eval():
  from botocore.exceptions import ClientError

  try:
    print("\t[INFO] Purging SQS eval queue... ", end='', flush=True)
    sqs = boto3.resource('sqs', region_name=_get_last('reader_region'))
    queue = sqs.get_queue_by_name(QueueName=CONNECTION_INFO['sqs_eval']['name'])
    queue.purge()
  except ClientError as e:
    if type(e).__name__ == 'PurgeQueueInProgress':
      print("Queue purge already in progress, try in 60seconds... ", end='', flush=True)
    else:
      raise

  print("Done!")

def _clean_service(role):
  from plumbum.cmd import aws

  print(f"\t[INFO] Deleting {role} stack... ", end='', flush=True)
  aws['cloudformation', 'delete-stack',
      '--region', _get_last(f"{role}_region"),
      '--stack-name', _get_last(f"{role}_stack_name"),
  ] & FG
  print("Done!")

def clean(args):
  # it actually means neither writer or reader got selected
  # hence we execute both
  if args['strong'] and (not args['writer'] and not args['reader']):
    args['writer'] = args['reader'] = True

  # if we passed writer or reader clean option that means its a strong clean for either
  if args['writer'] or args['reader']:
    args['strong'] = True

  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')

  print(f"[INFO] Cleaning '{post_storage}-{notification_storage}'... ")
  getattr(sys.modules[__name__], f"clean__{post_storage}")()
  getattr(sys.modules[__name__], f"clean__{notification_storage}")()
  _clean_sqs_eval()

  if args['strong']:
    if args['writer']:
      _clean_service('writer')

    if args['reader']:
      _clean_service('reader')

  # set flag that the experiment was cleaned
  _put_last('cleaned', True)
  print("[INFO] Done!")

def clean__mysql():
  import pymysql
  import pymysql.cursors

  MYSQL_CONN = CONNECTION_INFO['datastores']['mysql']

  # clean table before running lambda
  print("\t[INFO] Truncating MySQL table... ", end='', flush=True)
  mysql_conn = pymysql.connect(
      host=MYSQL_CONN[f"host__{_get_last('writer_region')}__writer"],
      port=MYSQL_CONN['port'],
      user=MYSQL_CONN['user'],
      password=MYSQL_CONN['password'],
      connect_timeout=60,
      autocommit=True
    )
  with mysql_conn.cursor() as cursor:
    try:
      sql = f"DROP DATABASE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
    except (pymysql.err.InternalError, pymysql.err.OperationalError) as e:
      code, msg = e.args
      if code == 1008:
        # database does not exist hence we ignore
        pass
      else:
        print(f"[WARN] MySQL error: {e}")
        exit(-1)

    try:
      sql = f"CREATE DATABASE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
      sql = f"USE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
    except pymysql.err.InternalError as e:
      print(f"[WARN] MySQL error: {e}")
      exit(-1)

    sql = f"CREATE TABLE `{MYSQL_CONN['post_table_name']}` (k BIGINT, v VARCHAR({NOTIFICATION_KEY_LEN}), b LONGBLOB)"
    cursor.execute(sql)
    mysql_conn.commit()
    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['post_table_name']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    sql = f"CREATE TABLE `{MYSQL_CONN['notifications_table_name']}` (k BIGINT, v VARCHAR({NOTIFICATION_KEY_LEN}))"
    cursor.execute(sql)
    mysql_conn.commit()
    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['notifications_table_name']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    sql = f"CREATE TABLE `{MYSQL_CONN['antipode_table']}` (cid VARCHAR(32))"
    cursor.execute(sql)
    mysql_conn.commit()
    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['antipode_table']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    print("Done!")

def clean__sns():
  # no need to clean sns
  None

def clean__dynamo():
  print("\t[INFO] Truncating Dynamo table... ", end='', flush=True)
  DYNAMO_CONN = CONNECTION_INFO['datastores']['dynamo']
  for zone in [_get_last('writer_region'), _get_last('reader_region')]:
    resource_dynamo = boto3.resource('dynamodb', region_name=zone)
    table_names = [
      DYNAMO_CONN['post_table_name'],
      DYNAMO_CONN['notifications_table_name']
    ]
    for table_name in table_names:
      table = resource_dynamo.Table(table_name)
      # Only way to clean a table in DynamoDB is to go key by key

      # get key attributes name
      table_key_names = [ key.get("AttributeName") for key in table.key_schema ]
      # only retrieve the keys for each item in the table (minimize data transfer)
      expression_projection = ", ".join('#' + key for key in table_key_names)
      expression_attributes = { '#'+key: key for key in table_key_names }
      # delete items per page
      page = table.scan(ProjectionExpression=expression_projection, ExpressionAttributeNames=expression_attributes)
      with table.batch_writer() as batch:
        while page["Count"] > 0:
          # Delete items in batches
          for item_keys in page["Items"]:
            batch.delete_item(Key=item_keys)
          # Fetch the next page
          if 'LastEvaluatedKey' in page:
            page = table.scan(
                ProjectionExpression=expression_projection,
                ExpressionAttributeNames=expression_attributes,
                ExclusiveStartKey=page['LastEvaluatedKey']
              )
          else:
            break

      # assert that has 0 items
      # cli_dynamo = boto3.client('dynamodb', region_name=zone)
      # ItemCount is not updated frequently - impossible to assert
      # table_describe = cli_dynamo.describe_table(TableName=table_name)
      # assert(table_describe['Table']['ItemCount'] == 0)
  print("Done!")


#--------------
# RUN
#--------------
def _writer_invoke(args):
  writer_region, writer_lambda_arn, i = args
  writer_client = boto3.client('lambda', region_name=writer_region)
  while True:
    try:
      response = writer_client.invoke(
        FunctionName=writer_lambda_arn,
        InvocationType='RequestResponse',
        Payload=json.dumps(_generate_payload(i)),
      )
      response_code = int(json.loads(response['Payload'].read()).get('statusCode', 500))
      if response_code == 200:
        break
    except Exception as e:
      print(f"[ERROR] Exception while invoking AWS Writer Lambda: {e}")
      exit(-1)

def run(args):
  # check if experiment was cleaned and if not we ask the user if he wants to continue
  if not _get_last('cleaned') and not click.confirm('[WARN] The experiment was not cleaned before hand! You might gather old results afterwards. Do you still want to continue?', default=False):
    exit(-1)

  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')
  writer_region = _get_last(f"writer_region")
  writer_lambda_arn = _get_last(f"writer__lambda__arn")

  print(f"[INFO] Running '{post_storage}-{notification_storage}' for #{args['requests']} ... ")
  # invoke writer in paralell
  pool = mp.Pool(processes=CPU_COUNT)
  for _ in tqdm(pool.imap_unordered(_writer_invoke, [ (writer_region, writer_lambda_arn, i) for i in range(args['requests']) ] ), total=args['requests']):
    pass
  pool.close()
  pool.join()

  # set run flags
  _put_last('cleaned', False)
  _put_last('requests', args['requests'])
  print("Done!")


#--------------
# GATHER
#--------------
def gather(args):
  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')

  # tag for this gather
  if args['tag'] is None:
    args['tag'] = input(f"Input any tag for this gather: ")

  # load build args
  reader_region = _get_last('reader_region')
  queue_url = CONNECTION_INFO['sqs_eval'][f"url__{reader_region}"]
  queue_url = CONNECTION_INFO['sqs_eval'][f"url__{reader_region}"]

  cli_sqs = boto3.client('sqs', region_name=reader_region)

  print("[INFO] Waiting for all messages to arrive at eval queue... ", flush=True)
  num_requests = _get_last('requests')
  num_messages = 0
  try:
    with tqdm(total=num_requests) as pbar:
      while True:
        reply = cli_sqs.get_queue_attributes(
            QueueUrl=queue_url,
            AttributeNames=[ 'ApproximateNumberOfMessages' ]
          )
        num_messages = int(reply['Attributes']['ApproximateNumberOfMessages'])
        # update pbar
        pbar.n = min(num_messages, num_requests)
        pbar.refresh()
        # break if necessary or sleep 1 second per message missing
        if num_messages >= num_requests:
          break
        else:
          time.sleep((num_requests-num_messages)/1000.0)
    print("Done!")
  except KeyboardInterrupt:
    print(f"[WARN] Skipping missing results {num_messages}/{num_requests}")
    # set new number of requests to messages
    num_requests = num_messages

  print("[INFO] Gater info through SQS... ", flush=True)
  resource_sqs = boto3.resource('sqs', region_name=reader_region)
  eval_queue = resource_sqs.get_queue_by_name(QueueName=CONNECTION_INFO['sqs_eval']['name'])

  # gather results - ideally all but if we cancel tqdm we move on with the ones we have
  results = {}
  try:
    with tqdm(total=num_requests) as pbar:
      while len(results) < num_requests:
        # fetch max 10 messages :(
        messages = eval_queue.receive_messages(MaxNumberOfMessages=10)
        # sleep if no messages returned from SQS
        if len(messages) == 0:
          time.sleep(10)
          continue

        for message in messages:
          # example entry:
          #   {'i': Decimal('7'), 'read_post_retries': Decimal('0'), 'ts_read_post_spent': Decimal('161')}

          # events might come from Lambda destinations (with responsePayload)
          # or sent manually inside lambdas
          item = json.loads(message.body)
          if 'responsePayload' in item:
            item = json.loads(item['responsePayload']['body'])

          # index by i so we avoid duplicate results
          results[int(item['i'])] = {
            'ts_notification_spent_ms': int(item['ts_notification_spent_ms']),
            'read_post_retries': int(item['read_post_retries']),
            'ts_read_post_spent_ms': int(item['ts_read_post_spent_ms']),
            'read_post_key_retries': int(item['read_post_key_retries']),
            'ts_read_post_key_spent_ms': int(item['ts_read_post_key_spent_ms']),
            'read_post_blob_retries': int(item['read_post_blob_retries']),
            'ts_read_post_blob_spent_ms': int(item['ts_read_post_blob_spent_ms']),
            'antipode_spent_ms': int(item['antipode_spent_ms']),
          }
          message.delete()
          pbar.update(1)
    print("[INFO] Done!")
  except KeyboardInterrupt:
    print(f"[WARN] Skipping remaining results {len(results)}/{num_requests}")


  print("[INFO] Parsing evaluation ...", end='')
  df = pd.DataFrame(results.values())

  # generate path to save the results
  gather_path = ROOT_PATH / 'gather' / f"{post_storage}-{notification_storage}"  / f"{args['tag']}-{time.strftime('%Y%m%d%H%M%S')}"
  os.makedirs(gather_path, exist_ok=True)
  csv_path = gather_path / 'traces.csv'
  info_path = gather_path / 'traces.info'

  # save to csv so we can plot a timeline later
  df.to_csv(csv_path, sep=';', mode='w')
  print(f"[INFO] Save '{csv_path}'")

  # save the pandas describe
  with open(info_path, 'w') as f:
    print(f"[NUMRESULTS] {len(results)}/{_get_last('requests')}", file=f)
    print("", file=f)
    print(df.describe(percentiles=PERCENTILES_TO_PRINT), file=f)
    print("", file=f)
    print(f"[PERINCONSISTENCIES] % inconsistencies: {len(df.query('read_post_retries > 0'))/float(len(df))}", file=f)
  print(f"[INFO] Save '{info_path}'\n")
  # print file to stdout
  with open(info_path, 'r') as f:
    print(f.read())


#--------------
# CONSTANTS
#--------------
ROOT_PATH = Path(os.path.abspath(os.path.dirname(sys.argv[0])))
LAST_INFO_FILE = ROOT_PATH / '.last.yml'
POST_STORAGE = {
  'mysql'
}
NOTIFICATION_STORAGE = [
  'sns',
  'dynamo'
]
CONNECTION_INFO = _load_yaml(ROOT_PATH / 'connection_info.yaml')
REGIONS = {
  'eu': 'eu-central-1',
  'us': 'us-east-1',
  'sg': 'ap-southeast-1',
}
CPU_COUNT = mp.cpu_count()
NOTIFICATION_KEY_LEN = 10
PERCENTILES_TO_PRINT = [.25, .5, .75, .90, .99]

#--------------
# CMD LINE
#--------------
if __name__ == '__main__':

  # parse arguments
  main_parser = argparse.ArgumentParser()

  # different commands
  subparsers = main_parser.add_subparsers(help='commands', dest='which')

  # build application
  build_parser = subparsers.add_parser('build', help='Build application')
  build_parser.add_argument('-ps', '--post-storage', required=True, choices=POST_STORAGE, help="Post Storage datastore")
  build_parser.add_argument('-ns', '--notification-storage', required=True, choices=NOTIFICATION_STORAGE, help="Notification Storage datastore")
  build_parser.add_argument('-w', '--writer', required=True, choices=REGIONS.keys(), help="Build writer on the specified region")
  build_parser.add_argument('-r', '--reader', required=True, choices=REGIONS.keys(), help="Build reader on the specified region")
  build_parser.add_argument('-ant', '--antipode', action='store_true', help="Enables antipode on the lambdas")

  # run application
  run_parser = subparsers.add_parser('run', help='Run application')
  run_parser.add_argument('-r', '--requests', type=int, default=1, help="Number of requests to run")

  # clean application
  clean_parser = subparsers.add_parser('clean', help='Clean application')
  clean_parser.add_argument('-s', '--strong', action='store_true', help="Delete SAM stacks & Lambdas")
  clean_parser.add_argument('-w', '--writer', action='store_true', help="Delete only the writer")
  clean_parser.add_argument('-r', '--reader', action='store_true', help="Delete only the reader")

  # run application
  gather_parser = subparsers.add_parser('gather', help='Gather eval')
  gather_parser.add_argument('-t', '--tag', type=str, default=None, help="Tags the gather")

  # parse args
  args = vars(main_parser.parse_args())
  command = args.pop('which')

  # check if the
  if command == 'run' and len(str(args['requests'])) > NOTIFICATION_KEY_LEN:
    main_parser.error(f"Maximum allowed key length of {NOTIFICATION_KEY_LEN} - {len(str(args['requests']))} given. Please edit this value.")

  # call parser method dynamically
  getattr(sys.modules[__name__], command)(args)