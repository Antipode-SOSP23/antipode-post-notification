#!/usr/bin/env python3

from pathlib import Path
from pprint import pprint
import os
import shutil
import sys
import argparse
import random
import string
import multiprocessing as mp
import boto3
import json
from datetime import datetime
from plumbum import FG, BG
import time
from tqdm import tqdm
import click
import pandas as pd
from pathlib import Path
import shutil

# pd.set_option('display.float_format', lambda x: '%.3f' % x)
pd.set_option('display.html.table_schema', True)
pd.set_option('display.precision', 2)
pd.set_option('max_columns', None)
pd.set_option('display.expand_frame_repr', False)

#--------------
# AWS SQS EVAL QUEUE
#--------------
#   1. Go to us-east-1 zone and to the AWS SQS dashboard
#   2. Create queue with the following parameters:
#       - Standard type
#       - name: antipode-lambda-notifications
#--------------

#--------------
# AMAZON VPC
#--------------
#   - As a tip use the same name for all objects, its easier to track. We use 'antipode-mq'
#
#   1. Create a VPC with a unique CIDR block
#       - *MAIN CONCERN*: Amazon MQ peering connection WILL NOT WORK ON OVERLAPPING CIDR BLOCS. Hence choose a unique one for each region VPC
#   2. After creating click on ACTIONS and enable DNS hostnames
#   3. Create a subnet with the full CIDR block
#   4. Go to Security Groups and select the default one.
#       - Inbound rules: Add 2 rules for ALL TRAFFIC to Any IPv4 (0.0.0.0/0) and IPv6. Make sure you have a rule for the same SG ID
#       - Outbout rules: Add 2 rules for ALL TRAFFIC to Any IPv4 (0.0.0.0/0) and IPv6. Make sure you have a rule for the same SG ID
#   5. Create an Internet Gateway
#       - After creating attach it to the VPC
#   6. Go to the created subnets default Route Table
#       - Add an entry for 0.0.0.0/0 to the created internet gateway
#   7. Go to Endpoints and create an entrypoint for AWS services needed. Make sure you select the correct VPC and Subnet
#       - Reader: SNS, SQS
#       - Writer: SNS, Dynamo (Gateway), ec2
#--------------

#--------------
# AWS AURORA GLOBAL CLUSTER
#--------------
#   1. Go to eu-central-1 zone
#   2. Go to RDS dashboard and click on "Create Database"
#   3. Select "Standard Create"
#       - Engine type: Amazon Aurora
#       - MySQL compatibility
#       - Provisioned
#       - Single Master
#       - Select a version that supports "Global Database" feature
#       - Select PRODUCTION template
#       - Cluster name: 'antipode-lambda-eu'
#       - Username: 'antipode' / Password: 'antipode'
#       - Choose a burstable instance class
#           * Tick "Include previous generations" for older and cheaper instances
#       - Do not create Multi-AZ deployment
#       - Public access: YES
#       - Choose 'allow-all' VPC group
#       - Database port: 3306
#       - Disable Encryption
#       - Disable Performance Insights
#       - Disable Enhanced monitoring
#       - Disable auto minor version upgrade
#       - Enable delete protection
#   4. Wait for all the instances to be created
#   5. Select the top level "Global Database". Click on Actions and "Add AWS region". You will get to a "Add Region" panel where you can setup the new replica:
#       - Secondary region: <region_name>
#       - Select lowest memory optimized machine
#       - Do not create multi-az deployment
#       - Select the default VPC
#           - DO NOT CHANGE antipode-mq to support RDS by adding more subnets
#       - Enable Public access
#       - Select the 'allow-all' VPC security group. If its not created, you should create with:
#           - ALL TRAFFIC open for all IPv4 and IPv6, in inbound and outbound
#           - Rule to allow itself - the security group - in inbound and outbound
#       - Select the AZ terminated in a (?? needed)
#       - Do not enable "read replica write forwarding"
#       - DB instance identifier: antipode-lambda-<region name>-instance
#       - DB cluster identifier: antipode-lambda-<region name>
#       - Disable Performance Insights
#       - Disable Monitoring
#       - Disable Auto minor version upgrade
#
# ref: https://docs.aws.amazon.com/AmazonRDS/latest/AuroraUserGuide/aurora-global-database.html
#--------------

#--------------
# AWS SNS GLOBAL
#--------------
#   1. Go to eu-central-1 zone and to the AWS SNS dashboard
#   2. Go to Topics and create a new one with the following parameters:
#       - Standard type
#       - name: antipode-lambda-notifications
#--------------

#--------------
# AWS S3 GLOBAL
#--------------
#   1. Go AWS dashboard and create buckets within all zones
#       - Name: antipode-lambda-posts-<region>
#       - Enabled versioning
#   2. Go the primary region and create replication from the bucket in that region other buckets
#       - Name: to-reader-<secondary region>
#       - Rule scope: apply to all objects
#       - On Destination click 'Browse S3' and find the bucket named: antipode-lambda-posts-<secondary region>
#       - Use the 'antipode-lambda-s3-admin' IAM role
#           - This is a rule that gives S3 admin access to operations needed
#       - Do not select RTC
#
# NOTE: we should also change the replication priority for each deployment (input on code and wait for changes in dashboard?)
#--------------

#--------------
# AWS DYNAMO
#--------------
#   1. On each region create for posts, notifications and cscopes
#       - For name check the connection_info file
#       - Select everything default
#   2. After created go to dashboard on the primary region and select Tables:
#       - For the 3 tables (posts, notifications, cscopes) do the following:
#           - Go to Global Tables
#           - Create replica to the desired region
#           - Double check in secondary region if tables got created
#--------------

#--------------
# AWS ELASTICACHE
#--------------
#   1. Create a global cluster. Start with the primary zone (if you are adding a zone to an existing cluster just go to the dashboard and add zone)
#      The properties are similar for the other zones you add to the cluster
#       - Name: antipode-lambda-<region>
#       - Port: 6379 (or the one you define in connection_info.yaml)
#       - Node type: cache.r5.large
#       - Num replicas: 1
#       - Create a new Subnet group:
#           - Name: antipode-lambda-<region>
#           - Select previously created VPC and Subnet group
#           - Select the AZ preference to the only AZ that should be there
#       - Select the default SG for the choosed VPC
#       - Disable backups
#
#   - WARN: you might have to create an EC2 instance on the zone and perform an initial request to "unlock" the zone for EC
#--------------

#--------------
# AWS MQ
#--------------
#   1. Using the previously created VCP, you have to add peering between the reader and writer zone
#      Check the following material for more details:
#      https://docs.aws.amazon.com/vpc/latest/peering/create-vpc-peering-connection.html
#      https://docs.aws.amazon.com/vpc/latest/peering/vpc-peering-routing.html
#       - *MAIN CONCERN*: WILL NOT WORK WITH VPCs WITH OVERLAPING CIDRS
#       - Go to the the secondary zone and create a new Peering Connection
#           - Name: antipode-mq-<primary>-<secondary> (e.g. antipode-mq-eu-us)
#           - Select the previously created VPC
#           - The select the primary zone and paste the previously created VPC id
#       - Go to the Peering Connections on the primary zone and accept the pending request (you might want to change the name as well)
#       - On both zones go to the Routing Table. We will match the pair the CIDR blocks
#           - On zone REGION-A add the entry: <REGION-B CIDR block> -> pcx-id (peering connection)
#           - On zone REGION-B add the entry: <REGION-A CIDR block> -> pcx-id (peering connection)
#      At the end of the whole setup, primary should have a configuration similar to this one:
#           50.0.0.0/16	local       (self)
#           51.0.0.0/16	pcx-id      (peering connection to secondary, e.g. eu-us)
#           52.0.0.0/16	pcx-id      (peering connection to secondary, e.g. eu-sg)
#           0.0.0.0/0	  igw-id      (internet gateway)
#           (you might have more entries from the endpoint configurations)
#      And the secondaries should have a configuration similar to this one:
#           52.0.0.0/16	local       (self)
#           50.0.0.0/16	pcx-id      (peering connection to primary, e.g. eu-sg)
#           0.0.0.0/0	  igw-id      (internet gateway)
#           (you might have more entries from the endpoint configurations)
#   2. Go the all the zones and create a broker with the following configuration:
#       - Engine: Apache ActiveMQ
#       - Single-instance broker
#       - Durability optimized
#       - Broker name: antipode-lambda-notifications-<region>
#       - Username: antipode
#       - Password: antipode1antipode
#       - Broker engine: 5.16.2
#       - Select to create a default configuration
#       - Select pre-created VPC config: antipode-mq
#       - Select pre-created Security group: antipode-mq
#       - Disable maintenance
#   3. Double check that you CAN access the public broker management dashboard
#   4. Go the the PRIMARY (writer) zone and edit the created configuration by uncommeting the networkConnectors blocks and replace with this (change the uris as needed):
#      ref: https://docs.aws.amazon.com/amazon-mq/latest/developer-guide/amazon-mq-creating-configuring-network-of-brokers.html
#
# <networkConnectors>
#   <networkConnector duplex="true" name="ConnectorEuToUs" uri="static:(ssl://b-6cfdfde0-2f84-4723-94bd-cc9ada66c2a9-1.mq.us-east-1.amazonaws.com:61617)" userName="antipode"/>
#   <networkConnector duplex="true" name="ConnectorEuToSg" uri="static:(ssl://b-6cfdfde0-2f84-4723-94bd-cc9ada66c2a9-1.mq.us-east-1.amazonaws.com:61617)" userName="antipode"/>
# </networkConnectors>
#
#   5. Go the broker again and change the REVISION of the configuration file and do APPLY IMMEDEATLY
#
#   6. Create a consumer on a secondary region to the primary region (change url):
# activemq consumer --brokerUrl "ssl://b-20f3cf89-7725-44b0-946b-19e84c03b81e-1.mq.ap-southeast-1.amazonaws.com:61617" \
#                   --user antipode \
#                   --password antipode1antipode \
#                   --destination queue://antipode-notifications
#
#       - Double check with a producer
# activemq producer --brokerUrl "ssl://b-8b026a92-1858-4a76-bc7a-7bfb25be209d-1.mq.eu-central-1.amazonaws.com:61617" \
#                   --user antipode \
#                   --password antipode1antipode \
#                   --destination queue://antipode-notifications \
#                   --persistent true \
#                   --messageSize 1000 \
#                   --messageCount 10
#
#       - Go the the dashboard and you should see 10 messages enqueued and dequeued
#
#   7. Create a secret for MQ lambda access on the primary region:
#         aws secretsmanager create-secret --region us-east-1 --name antipode-mq --secret-string '{"username": "antipode", "password": "antipode1antipode"}'
#       - After created edit the secret and replicate to secondary regions
#
#--------------


#--------------
# HELPERS
#--------------
def _generate_payload(i):
  key = str(i) + ''.join(random.choices(string.ascii_uppercase, k=1)) + ''.join(random.choices(string.ascii_uppercase + string.digits, k=NOTIFICATION_KEY_LEN - len(str(i)) - 1))
  return {
      'i': i,
      'key': key,
      'sent_at': datetime.utcnow().timestamp()
    }

def _load_yaml(path):
  import yaml
  with open(path, 'r') as f:
    return yaml.safe_load(f) or {}

def _put_last(k,v):
  import yaml
  doc = {}
  # if file exists parse yaml otherwise create empty dict to write to
  if Path(LAST_INFO_FILE).exists():
    doc = _load_yaml(LAST_INFO_FILE)
  # write new value and save to file
  doc[k] = v
  with open(LAST_INFO_FILE, 'w+') as f:
      yaml.safe_dump(doc, f, default_flow_style=False)

def _get_last(k):
  import yaml
  doc = _load_yaml(LAST_INFO_FILE)
  return doc.get(k)

def _build_app_args(post_storage, notification_storage):
  return {
      'writer': {
        'stack_name': f"antipode-lambda-{post_storage}-{notification_storage}-writer",
        'lambda_name': f"AntipodeLambda{post_storage.title()}{notification_storage.title()}Writer",
      },
      'reader': {
        'stack_name': f"antipode-lambda-{post_storage}-{notification_storage}-reader",
        'lambda_name': f"AntipodeLambda{post_storage.title()}{notification_storage.title()}Reader",
      },
      'vpc_required': (CONNECTION_INFO['datastores'][post_storage]['vpc_required'] or CONNECTION_INFO['datastores'][notification_storage]['vpc_required']),
    }

def _template_env_variables():
  env = {}

  # transform all datastore info to env var style
  for storage,details in CONNECTION_INFO['datastores'].items():
    for k,v in details.items():
      # replace hyphens due to AWS limitation
      env[f"{storage}_{k.replace('-','_')}".upper()] = v

  # Add SQS eval queue info as well
  for k,v in CONNECTION_INFO['sqs_eval'].items():
    # replace hyphens due to AWS limitation
    env[f"SQS_EVAL_{k.replace('-','_')}".upper()] = v

  return env


#--------------
# BUILD
#--------------
def _build_service(role, args):
  from jinja2 import Environment
  import textwrap
  from plumbum.cmd import sam

  app_args = _build_app_args(args['post_storage'], args['notification_storage'])
  lambda_name = app_args[role]['lambda_name']
  deploy_dir = ROOT_PATH / 'deploy' / role

  # save build variables for other commands
  _put_last(f"{role}_stack_name", app_args[role]['stack_name'])

  print(f"\t[INFO] Building {role} application... ", flush=True)
  # remove old building dirs
  shutil.rmtree(deploy_dir, ignore_errors=True)
  # create empty new ones
  os.makedirs(deploy_dir, exist_ok=True)
  os.chdir(deploy_dir)

  template = """
    AWSTemplateFormatVersion: '2010-09-09'
    Transform: 'AWS::Serverless-2016-10-31'
    Description: 'Antipode Lambda - {{ post_storage }}<>{{ notification_storage }} - {{ role }}'
    Resources:
      {{ lambda_name }}:
        Type: AWS::Serverless::Function
        Properties:
          Handler: {{ role }}.lambda_handler
          Runtime: python3.7
          Description: 'Antipode Lambda - {{ post_storage }}<>{{ notification_storage }} - {{ role }}'
          MemorySize: 128
          Timeout: 90
          Role: {{ iam_lambda_admin_role_arn }}
          {% if vpc_required %}
          VpcConfig:
            SubnetIds:
              - {{ vpc_subnet_id }}
            SecurityGroupIds:
              - {{ vpc_security_group_id }}
          {% endif %}
          {% if role == 'reader' %}{{ reader_event_source | indent(10, False) }}{% endif %}
          Environment:
            Variables:
              POST_STORAGE: {{ post_storage }}
              NOTIFICATION_STORAGE: {{ notification_storage }}
              WRITER_REGION: {{ writer_region }}
              READER_REGION: {{ reader_region }}
              ANTIPODE: {{ antipode_enabled }}
              DELAY_MS: {{ delay_ms }}
              #---
              {% for k,v in env_variables.items() %}{{ k }}: {{ v }}
              {% endfor %}
  """
  template_generated = Environment().from_string(template).render({
    'role': role,
    'lambda_name': lambda_name,
    'iam_lambda_admin_role_arn': CONNECTION_INFO['iam_lambda_admin_role_arn'],
    'post_storage': args['post_storage'],
    'notification_storage': args['notification_storage'],
    'writer_region': args['writer'],
    'reader_region': args['reader'],
    'antipode_enabled': int(args['antipode']),
    'delay_ms': args['delay'],
    # vpc configurations if needed
    'vpc_required': app_args['vpc_required'],
    'vpc_subnet_id': CONNECTION_INFO['lambda']['network'][args[role]]['subnet_id'],
    'vpc_security_group_id': CONNECTION_INFO['lambda']['network'][args[role]]['security_group_id'],
    # each notification storage has a different event source for the reader lambda
    'reader_event_source': getattr(sys.modules[__name__], f"reader_event_source__{args['notification_storage']}")(),
    # we pass everything and leave for lambda implementation to decide what and how to use
    'env_variables': _template_env_variables(),
  })
  with open('template.yaml', 'w') as f:
    # remove empty lines and dedent for easier read
    f.write(textwrap.dedent(template_generated).strip())

  # copy datastore lib files
  shutil.copy(ROOT_PATH / 'lambdas' / f"{role}.py", deploy_dir)
  shutil.copy(ROOT_PATH / 'lambdas' / args['post_storage'] / f"{args['post_storage']}.py", deploy_dir)
  shutil.copy(ROOT_PATH / 'lambdas' / args['notification_storage'] / f"{args['notification_storage']}.py", deploy_dir)

  # merge requirements files from all libs into one
  requirements_files = [
    ROOT_PATH / 'lambdas' / 'requirements.txt',
    ROOT_PATH / 'lambdas' / args['post_storage'] / 'requirements.txt',
    ROOT_PATH / 'lambdas' / args['notification_storage'] / 'requirements.txt',
  ]
  all_requirements = []
  for fname in requirements_files:
    with open(fname) as infile:
      all_requirements.append(infile.read())
  with open(deploy_dir / 'requirements.txt', 'w') as outfile:
    outfile.write('\n'.join(set(all_requirements)))

  # copy antipode code if its enabled
  if args['antipode']:
    shutil.copy(ROOT_PATH / 'lambdas' / 'antipode' / 'antipode.py', deploy_dir)
    shutil.copy(ROOT_PATH / 'lambdas' / 'antipode' / f"antipode_{args['post_storage']}.py", deploy_dir)

  # figure out deploy params
  region = args[role]
  s3_bucket = CONNECTION_INFO['lambda']['s3_buckets'][region]
  stack_name = app_args[role]['stack_name']
  lambda_name = app_args[role]['lambda_name']

  # Due to bug with SSL we comment this
  # sam['validate', '--region', region] & FG

  # DUE TO THIS BUG WE HAVE TO USE THIS --use-container FLAG -.-
  # https://github.com/aws/aws-lambda-builders/issues/229
  sam['build', '--use-container'] & FG
  sam['deploy',
      '--region', region,
      '--stack-name', stack_name,
      '--s3-bucket', s3_bucket,
      '--s3-prefix', stack_name,
      '--force-upload',
      '--profile', CONNECTION_INFO['aws_credentials_profile'],
      '--role-arn', CONNECTION_INFO['iam_cloudformation_admin_role_arn'],
      '--capabilities', 'CAPABILITY_IAM',
      '--no-confirm-changeset',
      '--no-fail-on-empty-changeset' # so if no changes this does not exit
    ] & FG
  # grab the arn of the deployed lambda
  cli_cloudformation = boto3.client('cloudformation', region_name=region)
  stack_deployed_lambda = cli_cloudformation.describe_stack_resource(StackName=stack_name, LogicalResourceId=lambda_name)['StackResourceDetail']
  cli_lambda = boto3.client('lambda', region_name=region)
  lambda_details = cli_lambda.get_function(FunctionName=stack_deployed_lambda['PhysicalResourceId'])
  _put_last(f"{role}__lambda__arn", lambda_details['Configuration']['FunctionArn'])

  os.chdir(ROOT_PATH)

def build(args):
  # save build variables for other commands
  _put_last('antipode_enabled', args['antipode'])
  _put_last('post_storage', args['post_storage'])
  _put_last('notification_storage', args['notification_storage'])
  # replace macro region with specific region
  # if we are only building one of the roles we get the last deployed role from file
  for role in ['reader', 'writer']:
    if not args[role]:
      args[role] = _get_last(f"{role}_region")
    else:
      args[role] = REGIONS[args[role]]
      _put_last(f"{role}_region", args[role])

  # trigger build for writer and reader
  print(f"[INFO] Building {args['post_storage']}-{args['notification_storage']} application... ", flush=True)
  _build_service('writer', args)
  _build_service('reader', args)
  print("Done!")

def reader_event_source__sns():
  from jinja2 import Environment
  import textwrap

  # SNS doesnt have global replication hence the reader gets the notifications straight from the writer region topic
  # an alternative would be more complex: create a queue to send messagest from SNS to SQS, cross-region subscribe and read in the lambda
  writer_region = _get_last(f"writer_region")

  template = """
    Events:
      SnsEvent:
        Type: SNS
        Properties:
          Region: {{ writer_region }}
          Topic: {{ sns_arn }}
  """
  template_generated = Environment().from_string(template).render({
    'writer_region': writer_region,
    'sns_arn': CONNECTION_INFO['datastores']['sns'][f"arn__{writer_region}__writer"],
  })
  return textwrap.dedent(template_generated).strip()

def reader_event_source__dynamo():
  from jinja2 import Environment
  import textwrap

  # each region has a different stream for the notification table
  reader_region = _get_last(f"reader_region")
  stream_arn = CONNECTION_INFO['datastores']['dynamo'][f"notifications_table_stream_arn__{reader_region}"]

  template = """
    Events:
      DynamoDbEvent:
        Type: DynamoDB
        Properties:
          BatchSize: 1
          StartingPosition: LATEST
          Stream: {{ notifications_table_stream_arn }}
  """
  template_generated = Environment().from_string(template).render({
    'notifications_table_stream_arn': stream_arn,
  })

  return textwrap.dedent(template_generated).strip()

def reader_event_source__mq():
  from jinja2 import Environment
  import textwrap

  # each region has a different stream for the notification table
  reader_region = _get_last(f"reader_region")
  broker_arn = CONNECTION_INFO['datastores']['mq'][f"arn__{reader_region}"]
  queue = CONNECTION_INFO['datastores']['mq']['notification_queue']
  secretmanager_arn = CONNECTION_INFO['datastores']['mq'][f"secretmanager_arn__{reader_region}"]

  template = """
    Events:
      MQEvent:
        Type: MQ
        Properties:
          Broker: {{ reader_broker_arn }}
          Queues:
            - {{ notifications_queue }}
          SourceAccessConfigurations:
            - Type: BASIC_AUTH
              URI: {{ secretmanager_arn }}
          BatchSize: 1
  """
  template_generated = Environment().from_string(template).render({
    'reader_broker_arn': broker_arn,
    'notifications_queue': queue,
    'secretmanager_arn': secretmanager_arn,
  })

  return textwrap.dedent(template_generated).strip()


#--------------
# CLEAN
#--------------
def _clean_sqs_eval():
  from botocore.exceptions import ClientError

  try:
    print("\t[INFO] Purging SQS eval queue... ", end='', flush=True)
    sqs = boto3.resource('sqs', region_name=_get_last('reader_region'))
    queue = sqs.get_queue_by_name(QueueName=CONNECTION_INFO['sqs_eval']['name'])
    queue.purge()
  except ClientError as e:
    if type(e).__name__ == 'PurgeQueueInProgress':
      print("Queue purge already in progress, try in 60seconds... ", end='', flush=True)
    else:
      raise

  print("Done!")

def _clean_service(role):
  from plumbum.cmd import aws

  print(f"\t[INFO] Deleting {role} stack... ", end='', flush=True)
  aws['cloudformation', 'delete-stack',
      '--region', _get_last(f"{role}_region"),
      '--stack-name', _get_last(f"{role}_stack_name"),
  ] & FG
  print("Done!")

def clean(args):
  # it actually means neither writer or reader got selected
  # hence we execute both
  if args['strong'] and (not args['writer'] and not args['reader']):
    args['writer'] = args['reader'] = True

  # if we passed writer or reader clean option that means its a strong clean for either
  if args['writer'] or args['reader']:
    args['strong'] = True

  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')

  print(f"[INFO] Cleaning '{post_storage}-{notification_storage}'... ")
  getattr(sys.modules[__name__], f"clean__{post_storage}")()
  getattr(sys.modules[__name__], f"clean__{notification_storage}")()
  _clean_sqs_eval()

  if args['strong']:
    if args['writer']:
      _clean_service('writer')

    if args['reader']:
      _clean_service('reader')

  # set flag that the experiment was cleaned
  _put_last('cleaned', True)
  print("[INFO] Done!")

def clean__mysql():
  import pymysql
  import pymysql.cursors

  MYSQL_CONN = CONNECTION_INFO['datastores']['mysql']

  # clean table before running lambda
  print("\t[INFO] Truncating MySQL table... ", end='', flush=True)
  mysql_conn = pymysql.connect(
      host=MYSQL_CONN[f"host__{_get_last('writer_region')}__writer"],
      port=MYSQL_CONN['port'],
      user=MYSQL_CONN['user'],
      password=MYSQL_CONN['password'],
      connect_timeout=60,
      autocommit=True
    )
  with mysql_conn.cursor() as cursor:
    try:
      sql = f"DROP DATABASE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
    except (pymysql.err.InternalError, pymysql.err.OperationalError) as e:
      code, msg = e.args
      if code == 1008:
        # database does not exist hence we ignore
        pass
      else:
        print(f"[WARN] MySQL error: {e}")
        exit(-1)

    try:
      sql = f"CREATE DATABASE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
      sql = f"USE `{MYSQL_CONN['db']}`"
      cursor.execute(sql)
    except pymysql.err.InternalError as e:
      print(f"[WARN] MySQL error: {e}")
      exit(-1)

    sql = f"CREATE TABLE `{MYSQL_CONN['post_table_name']}` (k BIGINT, v VARCHAR({NOTIFICATION_KEY_LEN}), b LONGBLOB)"
    cursor.execute(sql)
    mysql_conn.commit()
    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['post_table_name']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    sql = f"CREATE TABLE `{MYSQL_CONN['notifications_table_name']}` (k BIGINT, v VARCHAR({NOTIFICATION_KEY_LEN}))"
    cursor.execute(sql)
    mysql_conn.commit()
    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['notifications_table_name']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    sql = f"CREATE TABLE `{MYSQL_CONN['antipode_table']}` (cid VARCHAR(32))"
    cursor.execute(sql)
    mysql_conn.commit()
    sql = f"SELECT COUNT(*) FROM `{MYSQL_CONN['antipode_table']}`"
    cursor.execute(sql)
    assert(cursor.fetchone()[0] == 0)

    print("Done!")

def clean__sns():
  # no need to clean sns
  None

def clean__dynamo():
  print("\t[INFO] Truncating Dynamo table... ", end='', flush=True)
  DYNAMO_CONN = CONNECTION_INFO['datastores']['dynamo']
  for zone in [_get_last('writer_region'), _get_last('reader_region')]:
    resource_dynamo = boto3.resource('dynamodb', region_name=zone)
    table_names = [
      DYNAMO_CONN['post_table_name'],
      DYNAMO_CONN['notifications_table_name'],
      DYNAMO_CONN['antipode_table'],
    ]
    for table_name in table_names:
      table = resource_dynamo.Table(table_name)
      # Only way to clean a table in DynamoDB is to go key by key

      # get key attributes name
      table_key_names = [ key.get("AttributeName") for key in table.key_schema ]
      # only retrieve the keys for each item in the table (minimize data transfer)
      expression_projection = ", ".join('#' + key for key in table_key_names)
      expression_attributes = { '#'+key: key for key in table_key_names }
      # delete items per page
      page = table.scan(ProjectionExpression=expression_projection, ExpressionAttributeNames=expression_attributes)
      with table.batch_writer() as batch:
        while page["Count"] > 0:
          # Delete items in batches
          for item_keys in page["Items"]:
            batch.delete_item(Key=item_keys)
          # Fetch the next page
          if 'LastEvaluatedKey' in page:
            page = table.scan(
                ProjectionExpression=expression_projection,
                ExpressionAttributeNames=expression_attributes,
                ExclusiveStartKey=page['LastEvaluatedKey']
              )
          else:
            break

      # assert that has 0 items
      # cli_dynamo = boto3.client('dynamodb', region_name=zone)
      # ItemCount is not updated frequently - impossible to assert
      # table_describe = cli_dynamo.describe_table(TableName=table_name)
      # assert(table_describe['Table']['ItemCount'] == 0)
  print("Done!")

def clean__s3():
  print("\t[INFO] Cleaning S3 buckets... ", end='', flush=True)
  s3_resource = boto3.resource('s3')
  S3_CONN = CONNECTION_INFO['datastores']['s3']

  for role in ['reader', 'writer']:
    region = _get_last(f"{role}_region")
    bucket = S3_CONN[f"bucket__{region}__{role}"]
    s3_resource.Bucket(bucket).objects.all().delete()

  print("Done!")

def clean__mq():
  # nothing to do
  None

def clean__cache():
  print("\t[INFO] Cleaning Redis ElastiCache... ", end='', flush=True)
  # only way to clean cache is from within the VPC
  # hence we trigger a Lambda with a specific clean action tag
  writer_region = _get_last(f"writer_region")
  writer_lambda_arn = _get_last(f"writer__lambda__arn")

  writer_client = boto3.client('lambda', region_name=writer_region)
  response = writer_client.invoke(
      FunctionName=writer_lambda_arn,
      InvocationType='RequestResponse',
      Payload= json.dumps({ LAMBDA_CLEAN_TAG: 1 })
    )
  # exists if we get a 200 or 202 (for async) return code
  status_code = response['ResponseMetadata']['HTTPStatusCode']
  if status_code != 200:
    raise Exception('Failed to clean cache storage')
  print("Done!")

#--------------
# RUN
#--------------
def _writer_invoke(args):
  mode, writer_region, writer_lambda_arn, latencies, i = args
  writer_client = boto3.client('lambda', region_name=writer_region)
  try:
    response = writer_client.invoke(
        FunctionName=writer_lambda_arn,
        InvocationType=mode,
        Payload=json.dumps(_generate_payload(i)),
      )

    # save the time which we got a reply from
    latencies[i] = datetime.utcnow().timestamp()

    # exists if we get a 200 or 202 (for async) return code
    status_code = response['ResponseMetadata']['HTTPStatusCode']

    # replace status_code with the one frmo within the lambda
    if status_code == 200:
      status_code = int(json.loads(response['Payload'].read()).get('statusCode', 500))

    # now we parse the responses
    if status_code == 200 or status_code == 202:
      return
    else:
      raise Exception(f"Unknown Lambda return code: {status_code}")
  except Exception as e:
    print(f"[ERROR] Exception while invoking AWS Writer Lambda: {e}")
    exit(-1)

def run(args):
  # check if experiment was cleaned and if not we ask the user if he wants to continue
  if not _get_last('cleaned') and not click.confirm('[WARN] The experiment was not cleaned before hand! You might gather old results afterwards. Do you still want to continue?', default=False):
    exit(-1)

  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')
  writer_region = _get_last(f"writer_region")
  writer_lambda_arn = _get_last(f"writer__lambda__arn")
  mode = 'Event' if args['async'] else 'RequestResponse'

  with mp.Manager() as manager:
    # dict for requests latencies
    latencies = manager.dict()

    # build args for the invoke -- KEEP THE ORDER OF THE ARGUMENTS or adjust in the invoke method
    invoke_args_range = [ (mode, writer_region, writer_lambda_arn, latencies, i) for i in range(args['requests']) ]

    print(f"[INFO] Running '{post_storage}-{notification_storage}' for #{args['requests']} ... ")
    # invoke writer in paralell
    pool = mp.Pool(processes=CPU_COUNT)
    for _ in tqdm(pool.imap_unordered(_writer_invoke,  invoke_args_range), total=args['requests']):
      pass
    pool.close()
    pool.join()

    # save in last the latencies for each request
    _put_last('latencies', dict(latencies))

  _put_last('cleaned', False)
  _put_last('requests', args['requests'])
  print("Done!")


#--------------
# GATHER
#--------------
def gather(args):
  # load build vars
  post_storage = _get_last('post_storage')
  notification_storage = _get_last('notification_storage')
  antipode_enabled = _get_last('antipode_enabled')

  # tag for this gather
  if args['tag'] is None:
    args['tag'] = input('Input any tag for this gather: ')

  # load build args
  reader_region = _get_last('reader_region')
  queue_url = CONNECTION_INFO['sqs_eval'][f"url__{reader_region}"]
  queue_url = CONNECTION_INFO['sqs_eval'][f"url__{reader_region}"]

  cli_sqs = boto3.client('sqs', region_name=reader_region)

  num_requests = _get_last('requests')
  latencies = _get_last('latencies')
  if num_requests != len(latencies):
    print("[ERROR] Mismatch between number of requests and number of available latencies!")
    exit(-1)

  print("[INFO] Waiting for all messages to arrive at eval queue... ", flush=True)
  num_messages = 0
  try:
    with tqdm(total=num_requests) as pbar:
      while True:
        reply = cli_sqs.get_queue_attributes(
            QueueUrl=queue_url,
            AttributeNames=[ 'ApproximateNumberOfMessages' ]
          )
        num_messages = int(reply['Attributes']['ApproximateNumberOfMessages'])
        # update pbar
        pbar.n = min(num_messages, num_requests)
        pbar.refresh()
        # break if necessary or sleep 1 second per message missing
        if num_messages >= num_requests:
          break
        else:
          time.sleep((num_requests-num_messages)/1000.0)
    print("Done!")
  except KeyboardInterrupt:
    print(f"[WARN] Skipping missing results {num_messages}/{num_requests}")
    # set new number of requests to messages
    num_requests = num_messages

  print("[INFO] Gather info from SQS... ", flush=True)
  resource_sqs = boto3.resource('sqs', region_name=reader_region)
  eval_queue = resource_sqs.get_queue_by_name(QueueName=CONNECTION_INFO['sqs_eval']['name'])

  # gather results - ideally all but if we cancel tqdm we move on with the ones we have
  results = {}
  try:
    with tqdm(total=num_requests) as pbar:
      while len(results) < num_requests:
        # fetch max 10 messages :(
        messages = eval_queue.receive_messages(MaxNumberOfMessages=10)
        # sleep if no messages returned from SQS
        if len(messages) == 0:
          time.sleep(10)
          continue

        for message in messages:
          # events might come from Lambda destinations (with responsePayload)
          # or sent manually inside lambdas
          item = json.loads(message.body)
          if 'responsePayload' in item:
            item = json.loads(item['responsePayload']['body'])

          # index by i so we avoid duplicate results
          e = {
            'ts_notification_spent_ms': int(item['ts_notification_spent_ms']),
            'read_post_retries': int(item['read_post_retries']),
            'ts_read_post_spent_ms': int(item['ts_read_post_spent_ms']),
            #
            'post_read_at': float(item['post_read_at']),
            'sent_at': float(item['sent_at']),
            'reply_at': float(latencies[int(item['i'])]),
          }
          if antipode_enabled:
            e['antipode_spent_ms'] = int(item['antipode_spent_ms'])

          results[int(item['i'])] = e
          message.delete()
          pbar.update(1)
    print("[INFO] Done!")
  except KeyboardInterrupt:
    print(f"[WARN] Skipping remaining results {len(results)}/{num_requests}")

  # now we compute date over the global results and tidy up dict for dataframe
  eval_values = results.values()
  first_client_sent = min(eval_values, key=lambda k: k['sent_at'])['sent_at']
  last_client_replied = max(eval_values, key=lambda k: k['reply_at'])['reply_at']
  latest_post_read = max(eval_values, key=lambda k: k['post_read_at'])['post_read_at']

  # for each entry in dict we compute results and delete keys no longer needed
  for _,r in results.items():
    r['latency_ms'] = int((r['reply_at'] - r['sent_at']) * 1000)
    r['visibility_latency_ms'] = int((r['post_read_at'] - r['sent_at']) * 1000)
    # delete no longer needed keys
    del r['sent_at']
    del r['reply_at']
    del r['post_read_at']

  print("[INFO] Parsing evaluation ...", end='')
  df = pd.DataFrame(results.values())

  # generate path to save the results
  gather_path = ROOT_PATH / 'gather' / f"{post_storage}-{notification_storage}"  / f"{args['tag']}-{time.strftime('%Y%m%d%H%M%S')}"
  os.makedirs(gather_path, mode=0o777, exist_ok=True)
  csv_path = gather_path / 'traces.csv'
  info_path = gather_path / 'traces.info'

  # save to csv so we can plot a timeline later
  df.to_csv(csv_path, sep=';', mode='w')
  print(f"[INFO] Save '{csv_path}'")

  # figure out the throughput
  num_results = len(results)
  # from sent_at@i=0 to sent_at@i=0
  time_spent_by_client = last_client_replied - first_client_sent

  # values that we will write in the info file as tags for later parsing
  eval_tags = {
    'NUM_RESULTS': f"{num_results}/{_get_last('requests')}",
    '%_INCONSISTENCIES': len(df.query('read_post_retries > 0')) / float(len(df)),
    'THROUGHPUT_ON_CLIENT': num_results / float(time_spent_by_client),
  }

  # save the pandas describe
  with open(info_path, 'w') as f:
    for tag,v in eval_tags.items():
      print(f"[{tag}] {v}", file=f)
    print("", file=f)
    print(df.describe(percentiles=PERCENTILES_TO_PRINT), file=f)
  print(f"[INFO] Save '{info_path}'\n")
  # print file to stdout
  with open(info_path, 'r') as f:
    print(f.read())


#--------------
# CONSTANTS
#--------------
ROOT_PATH = Path(os.path.abspath(os.path.dirname(sys.argv[0])))
LAST_INFO_FILE = ROOT_PATH / '.last.yml'
#--------------
# post_storage METHODS:
#   - write_post(i, k) -> op
#   - read_post(k, evaluation)
#   - antipode_bridge(id, role) -> AntipodeBridge
#--------------
# notification_storage METHODS:
#   - write_notification(event)
#   - parse_event(event)
#--------------
POST_STORAGE = [
  'mysql',
  'dynamo',
  's3',
  'cache',
]
NOTIFICATION_STORAGE = [
  'sns',
  'dynamo',
  'mq',
]
CONNECTION_INFO = _load_yaml(ROOT_PATH / 'connection_info.yaml')
REGIONS = {
  'eu': 'eu-central-1',
  'us': 'us-east-1',
  'sg': 'ap-southeast-1',
}
LAMBDA_CLEAN_TAG = "-#CLEAN#-"
CPU_COUNT = mp.cpu_count()
NOTIFICATION_KEY_LEN = 10
PERCENTILES_TO_PRINT = [.25, .5, .75, .90, .99]

#--------------
# CMD LINE
#--------------
if __name__ == '__main__':

  # parse arguments
  main_parser = argparse.ArgumentParser()

  # different commands
  subparsers = main_parser.add_subparsers(help='commands', dest='which')

  # build application
  build_parser = subparsers.add_parser('build', help='Build application')
  build_parser.add_argument('-ps', '--post-storage', required=True, choices=POST_STORAGE, help="Post Storage datastore")
  build_parser.add_argument('-ns', '--notification-storage', required=True, choices=NOTIFICATION_STORAGE, help="Notification Storage datastore")
  build_parser.add_argument('-w', '--writer', required=True, choices=REGIONS.keys(), help="Build writer on the specified region")
  build_parser.add_argument('-r', '--reader', required=True, choices=REGIONS.keys(), help="Build reader on the specified region")
  build_parser.add_argument('-ant', '--antipode', action='store_true', help="Enables antipode on the lambdas")
  build_parser.add_argument('-d', '--delay', type=int, default=0, help="Apply ms delay before writing notification")

  # run application
  run_parser = subparsers.add_parser('run', help='Run application')
  run_parser.add_argument('-r', '--requests', type=int, default=1, help="Number of requests to run")
  # ref: https://docs.aws.amazon.com/lambda/latest/dg/invocation-async.html
  run_parser.add_argument('-a', '--async', action='store_true', help="Switches lambda invocation to async")

  # clean application
  clean_parser = subparsers.add_parser('clean', help='Clean application')
  clean_parser.add_argument('-s', '--strong', action='store_true', help="Delete SAM stacks & Lambdas")
  clean_parser.add_argument('-w', '--writer', action='store_true', help="Delete only the writer")
  clean_parser.add_argument('-r', '--reader', action='store_true', help="Delete only the reader")

  # run application
  gather_parser = subparsers.add_parser('gather', help='Gather eval')
  gather_parser.add_argument('-t', '--tag', type=str, default=None, help="Tags the gather")

  # parse args
  args = vars(main_parser.parse_args())
  command = args.pop('which')

  if command == 'run':
    # check if the number of requests is bigger than the range of notification keys available
    if len(str(args['requests'])) > NOTIFICATION_KEY_LEN:
      main_parser.error(f"Maximum allowed key length of {NOTIFICATION_KEY_LEN} - {len(str(args['requests']))} given. Please edit this value.")

  # call parser method dynamically
  getattr(sys.modules[__name__], command)(args)